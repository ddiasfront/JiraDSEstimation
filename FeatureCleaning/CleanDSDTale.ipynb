{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "029fad86-86c9-43bc-a9db-9d8124c3a830",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35d2209-3e5b-4cd7-a702-2eed1badf800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Database & File IO\n",
    "from pymongo import MongoClient\n",
    "import json5 as json\n",
    "\n",
    "# Standard Data Manipulation\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', None)  # We want to see all data\n",
    "from statistics import mean, median\n",
    "\n",
    "# Tracking Time\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e9fe79-fa28-42de-99d9-544d870e405f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb035275-5360-43cc-8dec-e7d1df4c7417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "db = client['JiraRepos']\n",
    "\n",
    "# Load the Jira Data Sources JSON\n",
    "with open('../0. DataDefinition/jira_data_sources.json') as f:\n",
    "    jira_data_sources = json.load(f)\n",
    "\n",
    "# Load the Jira Issue Types Information (Downloaded using the DataDownload script)\n",
    "with open('../0. DataDefinition/jira_issuetype_information.json') as f:\n",
    "    jira_issuetype_information = json.load(f)\n",
    "\n",
    "# Load the Jira Issue Link Types Information (Downloaded using the DataDownload script)\n",
    "with open('../0. DataDefinition/jira_issuelinktype_information.json') as f:\n",
    "    jira_issuelinktype_information = json.load(f)\n",
    "\n",
    "# Load the Jira Thematic Analysis JSON\n",
    "# with open('./jira_issuetype_thematic_analysis.json') as f:\n",
    "#     issuetype_themes_codes = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab9818d-84fa-4688-837a-f31fd84639dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Helpful Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e7eb28-2d8e-42c4-a5b6-d9a571d81977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ALL_JIRAS = [jira_name for jira_name in jira_data_sources.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad9a385-2310-41f4-8157-9644d2cd213b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1f407e3-d97b-4125-9723-35b613b42534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These are the global dataframes that we will perform our analysis on.\n",
    "df_jiras = pd.DataFrame(\n",
    "    np.nan,\n",
    "    columns=['Born', 'Issues', 'DIT', 'UIT', 'Links', 'DLT', 'ULT', 'Changes', 'Ch/I', 'UP', 'Comments', 'Co/I'],\n",
    "    index=ALL_JIRAS + ['Sum', 'Median', 'Std Dev']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da7ad6-860b-4e3b-b7df-72d77c258221",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query Data for Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30953763",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ed4378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing repository: Hyperledger ...\n",
      "Processing repository: SecondLife ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_1759/60208438.py:317: FutureWarning:\n",
      "\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed. Launching D-Tale session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2011-02-22 21:15:21', '2014-12-16 22:44:11', '2014-07-08 12:58:59',\n",
      " '2011-08-18 06:48:31', '2011-02-04 14:04:13', '2010-12-10 11:24:02',\n",
      " '2012-04-27 16:47:50', '2014-03-03 20:51:59', '2011-10-21 22:06:16',\n",
      " '2011-10-21 19:43:07',\n",
      " ...\n",
      " '2018-05-07 08:52:24', '2019-03-29 20:32:41', '2018-07-25 06:15:38',\n",
      " '2019-03-29 20:34:07', '2018-01-26 16:37:02', '2018-10-10 15:21:23',\n",
      " '2018-02-02 12:13:11', '2018-09-25 20:15:44', '2019-11-12 15:41:32',\n",
      " '2018-03-22 13:40:54']\n",
      "Length: 266, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2011-02-01 17:34:57', '2014-12-11 17:06:31', '2014-01-08 01:23:04',\n",
      " '2011-04-23 14:22:42', '2010-11-03 17:19:35', '2010-12-09 10:20:13',\n",
      " '2011-04-03 00:52:19', '2011-03-04 01:18:32', '2011-09-22 07:17:23',\n",
      " '2010-10-26 11:15:08',\n",
      " ...\n",
      " '2018-03-31 14:45:10', '2017-06-19 22:25:40', '2018-02-04 08:09:54',\n",
      " '2017-06-21 20:33:56', '2017-12-08 17:33:38', '2018-10-10 06:45:47',\n",
      " '2018-02-01 21:13:39', '2018-09-25 15:22:08', '2017-07-03 12:57:47',\n",
      " '2018-03-12 14:25:16']\n",
      "Length: 266, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2011-02-22 21:15:21', '2014-12-16 22:44:11', '2014-07-08 12:58:59',\n",
      " '2011-09-25 02:21:12', '2011-04-08 17:04:45', '2011-02-14 22:22:43',\n",
      " '2014-02-27 21:33:58', '2014-03-03 20:51:59', '2011-10-21 22:06:16',\n",
      " '2012-04-17 17:08:36',\n",
      " ...\n",
      " '2018-10-24 08:18:41', '2019-03-29 20:32:41', '2018-07-25 06:15:38',\n",
      " '2019-03-29 20:34:08', '2018-01-26 16:37:02', '2018-10-10 15:21:23',\n",
      " '2018-07-20 14:15:36', '2018-09-25 20:15:44', '2019-11-12 15:41:32',\n",
      " '2018-07-20 18:48:09']\n",
      "Length: 266, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ D-Tale session launched successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import dtale\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the Sentence Transformer model (you can choose a smaller one if needed)\n",
    "desc_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def parse_date_str(x):\n",
    "    \"\"\"\n",
    "    Parse a date string using dateparser. If the string is \"Missing\", empty, or cannot be parsed, return pd.NaT.\n",
    "    \"\"\"\n",
    "    if pd.isnull(x):\n",
    "        return pd.NaT\n",
    "    s = str(x).strip()\n",
    "    if s.lower() == \"missing\" or s == \"\":\n",
    "        return pd.NaT\n",
    "    \n",
    "    return x\n",
    "\n",
    "def convert_date_columns_dateparser(df, date_columns):\n",
    "    \"\"\"\n",
    "    Convert the specified date columns from string to datetime using our custom parse_date_str.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input DataFrame containing date strings.\n",
    "      date_columns (list): List of column names to convert.\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: The DataFrame with specified columns converted to datetime objects.\n",
    "    \"\"\"\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(parse_date_str)\n",
    "    return df\n",
    "\n",
    "def drop_invalid_dates(df, date_columns):\n",
    "    \"\"\"\n",
    "    Drop rows where any of the specified date columns are NaT.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input DataFrame.\n",
    "      date_columns (list): List of date column names to check.\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: DataFrame with rows dropped if any of the specified date columns are NaT.\n",
    "    \"\"\"\n",
    "    return df.dropna(subset=date_columns)\n",
    "\n",
    "\n",
    "def fix_data_types(df, numeric_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Convert DataFrame columns (stored as strings) to appropriate data types,\n",
    "    excluding any date formatting.\n",
    "\n",
    "    For each column that is not list-like:\n",
    "      - If at least `numeric_threshold` fraction of values can be converted to numeric,\n",
    "        the column is converted to a numeric dtype.\n",
    "      - Otherwise, the column is cast to 'category' dtype.\n",
    "    (Date-like strings remain as strings.)\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].apply(lambda x: isinstance(x, list)).any():\n",
    "            continue\n",
    "        numeric_series = pd.to_numeric(df[col], errors='coerce')\n",
    "        if numeric_series.notnull().mean() >= numeric_threshold:\n",
    "            df[col] = numeric_series\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "\n",
    "def flatten_histories(histories):\n",
    "    \"\"\"\n",
    "    Flatten a list of changelog history entries into a DataFrame.\n",
    "    Each row represents a single change item.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for history in histories:\n",
    "        history_id = history.get(\"id\")\n",
    "        author = history.get(\"author\", {}).get(\"name\")\n",
    "        created = history.get(\"created\")\n",
    "        items = history.get(\"items\", [])\n",
    "        for item in items:\n",
    "            rows.append({\n",
    "                \"changelog.history_id\": history_id,\n",
    "                \"changelog.author\": author,\n",
    "                \"changelog.created\": created,\n",
    "                \"changelog.field\": item.get(\"field\"),\n",
    "                \"changelog.fieldtype\": item.get(\"fieldtype\"),\n",
    "                \"changelog.from\": item.get(\"from\"),\n",
    "                \"changelog.fromString\": item.get(\"fromString\"),\n",
    "                \"changelog.to\": item.get(\"to\"),\n",
    "                \"changelog.toString\": item.get(\"toString\")\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def process_issue_histories(issue):\n",
    "    \"\"\"\n",
    "    Process a single issue's changelog histories:\n",
    "      - Flatten the histories.\n",
    "      - Apply type conversion.\n",
    "      - Add an 'issue_key' (using 'key' if available, else 'id') for merging.\n",
    "    \"\"\"\n",
    "    if \"changelog\" in issue and \"histories\" in issue[\"changelog\"]:\n",
    "        histories = issue[\"changelog\"][\"histories\"]\n",
    "        df_history = flatten_histories(histories)\n",
    "        df_history = fix_data_types(df_history)\n",
    "        df_history[\"issue_key\"] = issue.get(\"key\", issue.get(\"id\"))\n",
    "        return df_history\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_and_flatten_histories(issues):\n",
    "    \"\"\"\n",
    "    Extract and flatten changelog histories from a list of issues using parallel processing.\n",
    "    \"\"\"\n",
    "    flattened_histories = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_issue_histories, issue): issue for issue in issues}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None and not result.empty:\n",
    "                flattened_histories.append(result)\n",
    "    if flattened_histories:\n",
    "        return pd.concat(flattened_histories, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def summarize_changelog_histories(df_histories):\n",
    "    \"\"\"\n",
    "    Summarize flattened changelog histories by counting the number of changes per field.\n",
    "    Returns a DataFrame with one row per issue (keyed by 'issue_key').\n",
    "    \"\"\"\n",
    "    summary = df_histories.groupby('issue_key')['changelog.field'].value_counts().unstack(fill_value=0).reset_index()\n",
    "    summary = summary.rename(columns=lambda x: f'changelog_count_{x}' if x != 'issue_key' else x)\n",
    "    return summary\n",
    "\n",
    "\n",
    "def drop_zero_dominated_columns(df, prefix='changelog_count_', zero_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Drop numeric columns with names starting with `prefix` if more than `zero_threshold`\n",
    "    fraction of their values are zeros.\n",
    "    \"\"\"\n",
    "    cols_to_drop = []\n",
    "    for col in df.columns:\n",
    "        if col.startswith(prefix) and df[col].dtype.kind in 'biufc':\n",
    "            frac_zeros = (df[col] == 0).mean()\n",
    "            if frac_zeros > zero_threshold:\n",
    "                cols_to_drop.append(col)\n",
    "    if cols_to_drop:\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_issue_links(issuelinks):\n",
    "    \"\"\"\n",
    "    Process the 'fields.issuelinks' JSON array and extract features:\n",
    "      - Total number of links.\n",
    "      - Count and binary flag for each link type.\n",
    "    \"\"\"\n",
    "    features = {\"issuelinks_total\": 0}\n",
    "    link_types = {}\n",
    "    if isinstance(issuelinks, list):\n",
    "        features[\"issuelinks_total\"] = len(issuelinks)\n",
    "        for link in issuelinks:\n",
    "            lt = link.get(\"type\", {}).get(\"name\", \"Unknown\")\n",
    "            link_types[lt] = link_types.get(lt, 0) + 1\n",
    "    else:\n",
    "        features[\"issuelinks_total\"] = 0\n",
    "    for lt, count in link_types.items():\n",
    "        features[f\"issuelinks_{lt.lower()}_count\"] = count\n",
    "        features[f\"has_issuelinks_{lt.lower()}\"] = 1 if count > 0 else 0\n",
    "    return features\n",
    "\n",
    "\n",
    "def process_comments(comments):\n",
    "    \"\"\"\n",
    "    Process the 'fields.comments' JSON array and extract summary features:\n",
    "      - Total number of comments.\n",
    "      - Average and maximum comment length.\n",
    "      - Number of unique authors.\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        \"comment_count\": 0,\n",
    "        \"avg_comment_length\": 0,\n",
    "        \"max_comment_length\": 0,\n",
    "        \"unique_authors_count\": 0\n",
    "    }\n",
    "    if not isinstance(comments, list) or len(comments) == 0:\n",
    "        return features\n",
    "    comment_bodies = [c.get('body', '') for c in comments if isinstance(c, dict)]\n",
    "    authors = [c.get('author', {}).get('name') for c in comments if isinstance(c, dict)]\n",
    "    features[\"comment_count\"] = len(comment_bodies)\n",
    "    lengths = [len(body) for body in comment_bodies]\n",
    "    if lengths:\n",
    "        features[\"avg_comment_length\"] = sum(lengths) / len(lengths)\n",
    "        features[\"max_comment_length\"] = max(lengths)\n",
    "    unique_authors = {a for a in authors if a is not None}\n",
    "    features[\"unique_authors_count\"] = len(unique_authors)\n",
    "    return features\n",
    "\n",
    "\n",
    "def process_description_field(descriptions):\n",
    "    \"\"\"\n",
    "    Process the 'fields.description' field by generating dense embeddings\n",
    "    using a pre-trained Sentence Transformer. The resulting embedding vector\n",
    "    is expanded into multiple columns (one per dimension).\n",
    "    \"\"\"\n",
    "    descriptions = descriptions.fillna(\"\").astype(str)\n",
    "    embeddings = descriptions.apply(lambda x: desc_model.encode(x, show_progress_bar=False))\n",
    "    emb_array = np.vstack(embeddings.values)\n",
    "    emb_df = pd.DataFrame(emb_array, index=descriptions.index,\n",
    "                          columns=[f\"desc_emb_{i}\" for i in range(emb_array.shape[1])])\n",
    "    return emb_df\n",
    "\n",
    "\n",
    "def process_repo(jira_name, db, sample_ratio):\n",
    "    \"\"\"\n",
    "    Process a single Jira repository:\n",
    "      - Load issues from MongoDB.\n",
    "      - Sample a fraction of issues.\n",
    "      - Flatten main issue data and apply type conversion.\n",
    "      - Extract and flatten changelog histories, then summarize them (without from/to transitions).\n",
    "      - Merge the changelog summary with the main DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing repository: {jira_name} ...\")\n",
    "    issues = list(db[jira_name].find())\n",
    "    if not issues:\n",
    "        print(f\"⚠️ No documents found for '{jira_name}', skipping.\")\n",
    "        return None\n",
    "    sample_size = max(1, int(len(issues) * sample_ratio))\n",
    "    sampled_issues = random.sample(issues, sample_size)\n",
    "    \n",
    "    df_main = pd.json_normalize(sampled_issues, sep='.')\n",
    "    df_main = fix_data_types(df_main)\n",
    "    \n",
    "    df_histories = extract_and_flatten_histories(sampled_issues)\n",
    "    if not df_histories.empty:\n",
    "        changelog_summary = summarize_changelog_histories(df_histories)\n",
    "        if \"key\" not in df_main.columns:\n",
    "            df_main[\"key\"] = df_main[\"id\"]\n",
    "        df_main = pd.merge(df_main, changelog_summary, how=\"left\", left_on=\"key\", right_on=\"issue_key\")\n",
    "        df_main.drop(columns=[\"issue_key\"], inplace=True, errors='ignore')\n",
    "    \n",
    "    return df_main\n",
    "\n",
    "\n",
    "def drop_high_missing_columns(df, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Drop columns from the DataFrame where the fraction of missing values exceeds the threshold.\n",
    "    \"\"\"\n",
    "    return df.loc[:, df.isnull().mean() <= threshold]\n",
    "\n",
    "\n",
    "def impute_missing_values(df, numeric_strategy='median', categorical_strategy='constant', fill_value='Missing'):\n",
    "    \"\"\"\n",
    "    Impute missing values using scikit-learn's SimpleImputer.\n",
    "      - Numeric columns: impute with the specified strategy (default: median).\n",
    "      - Categorical columns: impute with a constant value (default: \"Missing\").\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        num_imputer = SimpleImputer(strategy=numeric_strategy)\n",
    "        df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
    "    if len(categorical_cols) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy=categorical_strategy, fill_value=fill_value)\n",
    "        df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "    return df\n",
    "\n",
    "\n",
    "def explore_all_fields_in_dtale(selected_jiras=None, sample_ratio=0.2, missing_threshold=0.3,\n",
    "                                zero_threshold=0.8, open_dtale=True):\n",
    "    \"\"\"\n",
    "    Connect to the MongoDB 'JiraRepos' database, sample issues from selected repositories,\n",
    "    process and flatten changelog histories (summarizing them without from/to transitions),\n",
    "    process JSON array fields (issuelinks, comments) into engineered features,\n",
    "    process the 'fields.description' field into dense embedding features,\n",
    "    drop columns with excessive missing data, impute missing values,\n",
    "    and drop changelog summary columns dominated by zeros.\n",
    "    \n",
    "    If open_dtale is True, launch a D-Tale session for interactive visualization;\n",
    "    otherwise, simply return the final DataFrame.\n",
    "    \"\"\"\n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "    db = client[\"JiraRepos\"]\n",
    "\n",
    "    # Load Jira data sources configuration\n",
    "    with open(\"../0. DataDefinition/jira_data_sources.json\") as f:\n",
    "        jira_data_sources = json.load(f)\n",
    "\n",
    "    all_jiras = list(jira_data_sources.keys())\n",
    "    if selected_jiras and len(selected_jiras) > 0:\n",
    "        all_jiras = [j for j in all_jiras if j in selected_jiras]\n",
    "        if not all_jiras:\n",
    "            print(f\"⚠️ No valid Jira repositories found for {selected_jiras}.\")\n",
    "            return\n",
    "\n",
    "    # Process each repository in parallel\n",
    "    merged_dfs = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_repo, jira_name, db, sample_ratio): jira_name for jira_name in all_jiras}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                merged_dfs.append(result)\n",
    "\n",
    "    if merged_dfs:\n",
    "        final_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No data to display.\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    # Define columns to drop – these include system URLs, avatar URLs, raw MongoDB identifiers, etc.\n",
    "    cols_to_drop = [\n",
    "        \"_id\", \"expand\",\n",
    "        # System URLs and redundant identifiers\n",
    "        \"self\",\n",
    "        # FixVersions and versions (if not used for estimation)\n",
    "        \"fields.fixVersions\", \"fields.versions\",\n",
    "        # Avatar URLs and similar, since they are user-specific noise\n",
    "        \"fields.assignee.avatarUrls.48x48\", \"fields.assignee.avatarUrls.24x24\",\n",
    "        \"fields.assignee.avatarUrls.16x16\", \"fields.assignee.avatarUrls.32x32\",\n",
    "        \"fields.reporter.avatarUrls.48x48\", \"fields.reporter.avatarUrls.24x24\",\n",
    "        \"fields.reporter.avatarUrls.16x16\", \"fields.reporter.avatarUrls.32x32\",\n",
    "        \"fields.creator.avatarUrls.48x48\", \"fields.creator.avatarUrls.24x24\",\n",
    "        \"fields.creator.avatarUrls.16x16\", \"fields.creator.avatarUrls.32x32\",\n",
    "        # Some of the raw MongoDB fields\n",
    "        \"changelog.startAt\", \"changelog.maxResults\", \"changelog.total\",\n",
    "        # Votes and similar rarely useful fields\n",
    "        \"fields.votes.self\", \"fields.votes.hasVoted\"\n",
    "    ]\n",
    "\n",
    "    # Only drop columns that exist in the DataFrame.\n",
    "    existing_cols = [col for col in cols_to_drop if col in final_df.columns]\n",
    "    final_df = final_df.drop(columns=existing_cols, errors='ignore')\n",
    "\n",
    "    # Drop columns with high missing ratios\n",
    "    final_df = drop_high_missing_columns(final_df, threshold=missing_threshold)\n",
    "\n",
    "    # Process JSON array field for issuelinks\n",
    "    if \"fields.issuelinks\" in final_df.columns:\n",
    "        issuelinks_features = final_df[\"fields.issuelinks\"].apply(process_issue_links)\n",
    "        issuelinks_df = pd.json_normalize(issuelinks_features)\n",
    "        final_df = pd.concat([final_df.drop(columns=[\"fields.issuelinks\"]), issuelinks_df], axis=1)\n",
    "\n",
    "    # Process JSON array field for comments\n",
    "    if \"fields.comments\" in final_df.columns:\n",
    "        comments_features = final_df[\"fields.comments\"].apply(process_comments)\n",
    "        comments_df = pd.json_normalize(comments_features)\n",
    "        final_df = pd.concat([final_df.drop(columns=[\"fields.comments\"]), comments_df], axis=1)\n",
    "\n",
    "    # Process the 'fields.description' field to generate dense embeddings\n",
    "    if \"fields.description\" in final_df.columns:\n",
    "        desc_embeddings = process_description_field(final_df[\"fields.description\"])\n",
    "        final_df = pd.concat([final_df.drop(columns=[\"fields.description\"]), desc_embeddings], axis=1)\n",
    "\n",
    "    # Impute missing values\n",
    "    final_df = impute_missing_values(final_df)\n",
    "\n",
    "    # Drop changelog summary columns dominated by zeros\n",
    "    final_df = drop_zero_dominated_columns(final_df, prefix='changelog_count_', zero_threshold=zero_threshold)\n",
    "\n",
    "    date_cols = [\"fields.created\", \"fields.updated\", \"fields.resolutiondate\"]\n",
    "\n",
    "    final_df = convert_date_columns_dateparser(final_df, date_cols)\n",
    "    final_df = drop_invalid_dates(final_df, date_cols)\n",
    "    final_df['fields.created'] = pd.to_datetime(final_df['fields.created'], errors=\"coerce\", utc=True)\n",
    "    final_df['fields.updated'] = pd.to_datetime(final_df['fields.updated'], errors=\"coerce\", utc=True)\n",
    "    final_df['fields.resolutiondate'] = pd.to_datetime(final_df['fields.resolutiondate'], errors=\"coerce\", utc=True)\n",
    "\n",
    "    if open_dtale:\n",
    "        print(\"Data processed. Launching D-Tale session...\")\n",
    "        d = dtale.show(final_df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "        d.open_browser()\n",
    "        print(\"✅ D-Tale session launched successfully.\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def export_clean_df():\n",
    "    \"\"\"\n",
    "    Run the full OverviewAnalysis pipeline and return the final DataFrame with all engineered features.\n",
    "    This version does not launch D-Tale (open_dtale=False) so that it can be used directly as the training dataset.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The final processed DataFrame ready for training.\n",
    "    \"\"\"\n",
    "    final_df = explore_all_fields_in_dtale(\n",
    "        selected_jiras=[\"Hyperledger\", \"SecondLife\"],\n",
    "        sample_ratio=0.01,\n",
    "        missing_threshold=0.3,\n",
    "        zero_threshold=0.8,\n",
    "        open_dtale=True\n",
    "    )\n",
    "    return final_df\n",
    "\n",
    "\n",
    "# For testing purposes, you can run export_clean_df() if executing this module directly.\n",
    "if __name__ == \"__main__\":\n",
    "    df_for_training = export_clean_df()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
