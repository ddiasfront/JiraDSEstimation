{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "029fad86-86c9-43bc-a9db-9d8124c3a830",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35d2209-3e5b-4cd7-a702-2eed1badf800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Database & File IO\n",
    "from pymongo import MongoClient\n",
    "import json5 as json\n",
    "\n",
    "# Standard Data Manipulation\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', None)  # We want to see all data\n",
    "from statistics import mean, median\n",
    "\n",
    "# Tracking Time\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e9fe79-fa28-42de-99d9-544d870e405f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb035275-5360-43cc-8dec-e7d1df4c7417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "db = client['JiraRepos']\n",
    "\n",
    "# Load the Jira Data Sources JSON\n",
    "with open('../0. DataDefinition/jira_data_sources.json') as f:\n",
    "    jira_data_sources = json.load(f)\n",
    "\n",
    "# Load the Jira Issue Types Information (Downloaded using the DataDownload script)\n",
    "with open('../0. DataDefinition/jira_issuetype_information.json') as f:\n",
    "    jira_issuetype_information = json.load(f)\n",
    "\n",
    "# Load the Jira Issue Link Types Information (Downloaded using the DataDownload script)\n",
    "with open('../0. DataDefinition/jira_issuelinktype_information.json') as f:\n",
    "    jira_issuelinktype_information = json.load(f)\n",
    "\n",
    "# Load the Jira Thematic Analysis JSON\n",
    "# with open('./jira_issuetype_thematic_analysis.json') as f:\n",
    "#     issuetype_themes_codes = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab9818d-84fa-4688-837a-f31fd84639dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Helpful Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e7eb28-2d8e-42c4-a5b6-d9a571d81977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ALL_JIRAS = [jira_name for jira_name in jira_data_sources.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad9a385-2310-41f4-8157-9644d2cd213b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1f407e3-d97b-4125-9723-35b613b42534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These are the global dataframes that we will perform our analysis on.\n",
    "df_jiras = pd.DataFrame(\n",
    "    np.nan,\n",
    "    columns=['Born', 'Issues', 'DIT', 'UIT', 'Links', 'DLT', 'ULT', 'Changes', 'Ch/I', 'UP', 'Comments', 'Co/I'],\n",
    "    index=ALL_JIRAS + ['Sum', 'Median', 'Std Dev']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da7ad6-860b-4e3b-b7df-72d77c258221",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query Data for Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af17b5f7-adea-462a-a6bd-e0bf36290781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def populate_df_jiras(df_jiras, jiras=ALL_JIRAS):\n",
    "    \n",
    "    def extract_number_of_issues(jira_name):\n",
    "        # Query for the count of all issues\n",
    "        num_issues = db[jira_name].count_documents({})\n",
    "        # Return value\n",
    "        return num_issues\n",
    "            \n",
    "    def extract_number_of_documented_issuetypes(jira_name):\n",
    "        # Extract the number of documented issue types from the downloaded issuetype_information JSON downloaded earlier, and return\n",
    "        return len(jira_issuetype_information[jira_name])\n",
    "        \n",
    "    def extract_number_of_used_issuetypes(jira_name):\n",
    "        # Query for unique set of issuetypes in the final state of the issue\n",
    "        query_result = list(db[jira_name].aggregate([\n",
    "            # We only need the issuetype name for the final state evaluation\n",
    "            { '$project': { '_id': 0, 'issuetype_name': '$fields.issuetype.name' } },\n",
    "            # Create a unique set of these names\n",
    "            { '$group': { '_id': None, 'issuetype_names': { '$addToSet': '$issuetype_name' } } }\n",
    "        ]))\n",
    "        # Extract the query\n",
    "        unique_issuetypes_final = set(query_result[0]['issuetype_names']) if query_result else set()\n",
    "        # Query for unique set of issuetypes in the issue history\n",
    "        query_result = list(db[jira_name].aggregate([\n",
    "            # Unwind the histories and items to work with individual change items\n",
    "            { '$unwind': '$changelog.histories' },\n",
    "            { '$unwind': '$changelog.histories.items' },\n",
    "            # We only want the changes to the 'issuetype' field\n",
    "            { '$match': { 'changelog.histories.items.field': 'issuetype' } },\n",
    "            # Select and rename the nested 'fromString' attribute. We only care what the issueType was BEFORE changing.\n",
    "            # We have the subsequent 'toString' values in the next change 'fromString' or the final state extracted above.\n",
    "            { '$project': { '_id': 0, 'issuetype_name': '$changelog.histories.items.fromString' } },\n",
    "            # Create a unique set of these names\n",
    "            { '$group': { '_id': None, 'issuetype_names': { '$addToSet': '$issuetype_name' } } }\n",
    "        ]))\n",
    "        # Extract the query\n",
    "        unique_issuetypes_history = set(query_result[0]['issuetype_names']) if query_result else 0\n",
    "        # Union the two sets together, and count the items, and return\n",
    "        return len(set.union(unique_issuetypes_final, unique_issuetypes_history))\n",
    "    \n",
    "    def extract_number_of_issuelinks(jira_name):\n",
    "        # Extract the issuelinks\n",
    "        issuelinks_result = list(db[jira_name].aggregate([\n",
    "            # Limit to issues with issuelinks\n",
    "            { '$match': { 'fields.issuelinks': { '$exists': True, '$ne': [] } } },\n",
    "            # Limit the object data to just the issuelink ids, and rename/condense into a single field\n",
    "            { '$project': { '_id': 0, 'issuelink_ids_issue': '$fields.issuelinks.id' } },\n",
    "            # Create a new \"row\" for each issue link, since issues can have multiple issuelinks each\n",
    "            { '$unwind': '$issuelink_ids_issue' },\n",
    "            # Create a unique set of issuelink ids. Issuelinks link multiple issues together, but we only want to count this link once.\n",
    "            { '$group': { '_id': None, 'issuelink_unique_ids': { '$addToSet': '$issuelink_ids_issue' } } }\n",
    "        ]))\n",
    "        num_issuelinks = len(set(issuelinks_result[0]['issuelink_unique_ids'])) if issuelinks_result else 0\n",
    "        # Extract the subtasks\n",
    "        subtasks_result = list(db[jira_name].aggregate([\n",
    "            # Limit to issues with subtasks\n",
    "            { '$match': { 'fields.subtasks': { '$exists': True, '$ne': [] } } },\n",
    "            # Limit the object data to just the size of the subtask arrays.\n",
    "            { '$project': { '_id': 0, 'num_issue_subtasks': { '$size': '$fields.subtasks' } } },\n",
    "            # Count the subtask arrays across the entire jira dataset\n",
    "            { '$group': { '_id': None, 'num_subtasks': { '$sum': '$num_issue_subtasks' } } }\n",
    "        ]))\n",
    "        num_subtasks = subtasks_result[0]['num_subtasks'] if subtasks_result else 0\n",
    "        # Extract the epic links\n",
    "        epiclinkfield_dict = {\n",
    "            'Apache': 'customfield_12311120',\n",
    "            'Hyperledger': 'customfield_10006',\n",
    "            'IntelDAOS': 'customfield_10092',\n",
    "            'JFrog': 'customfield_10806',\n",
    "            'Jira': 'customfield_12931',\n",
    "            'JiraEcosystem': 'customfield_12180',\n",
    "            'MariaDB': 'customfield_10600',\n",
    "            'Mindville': 'customfield_10000',\n",
    "            'Mojang': 'customfield_11602',\n",
    "            'MongoDB': 'customfield_10857',\n",
    "            'Qt': 'customfield_10400',\n",
    "            'RedHat': 'customfield_12311140',\n",
    "            'Sakai': 'customfield_10772',\n",
    "            'SecondLife': 'customfield_10871',\n",
    "            'Sonatype': 'customfield_11500',\n",
    "            'Spring': 'customfield_10680'\n",
    "        }\n",
    "        epiclinks_result = list(db[jira_name].aggregate([\n",
    "            # Rename the field since every Jira uses a different customfield name\n",
    "            { '$project': { 'epiclink_field': f\"$fields.{epiclinkfield_dict[jira_name]}\" } },\n",
    "            # Limit to issues with epiclink fields\n",
    "            { '$match': { 'epiclink_field': { '$exists': True, '$ne': None } } },\n",
    "            # Count the number of records in the aggregation\n",
    "            { '$count': 'num_epiclinks' }\n",
    "        ]))\n",
    "        num_epiclinks = epiclinks_result[0]['num_epiclinks'] if epiclinks_result else 0  # Some repos have no epiclinks, so we need to catch this\n",
    "        # Total the number of issuelinks by summing the three values above, and return\n",
    "        return sum([num_issuelinks, num_subtasks, num_epiclinks])\n",
    "    \n",
    "    def extract_number_of_documented_issuelinktypes(jira_name):\n",
    "        # Extract the number of documented issue link types from the downloaded issuelinktype_information JSON downloaded earlier, and return\n",
    "        return len(jira_issuelinktype_information[jira_name]) if jira_name in jira_issuelinktype_information else 0\n",
    "    \n",
    "    def extract_number_of_used_issuelinktypes(jira_name):\n",
    "        # Query for unique set of issuelinktypes in the final state of the issue\n",
    "        query_result = list(db[jira_name].aggregate([\n",
    "            # Unwind the issuelinks into individual records\n",
    "            { '$unwind': '$fields.issuelinks' },\n",
    "            # Select and rename the issuelink type name to prepare for the group operator\n",
    "            { '$project': { '_id': 0, 'issuelinktype_name': '$fields.issuelinks.type.name' } },\n",
    "            # Create a unique set of the issuelink type names\n",
    "            { '$group': { '_id': None, 'issuelinktype_names': { '$addToSet': '$issuelinktype_name' } } }\n",
    "        ]))\n",
    "        # Extract the query, and return value\n",
    "        return len(set(query_result[0]['issuelinktype_names'])) if query_result else 0\n",
    "    \n",
    "    def extract_born(jira_name):\n",
    "        # Get the first N issues in each repo to check for the initial \"birth\" of the repo\n",
    "        created_dates = [issue['fields']['created'] for issue in\n",
    "            db[jira_name].aggregate([\n",
    "                # We only need the created field\n",
    "                { '$project': { '_id': 0, 'fields.created': 1 } },\n",
    "                # Sort the items by created date (ascending) to get the earliest dates first\n",
    "                { '$sort': { 'fields.created': 1 } },\n",
    "                # We only technically need the first item, but practically there are issues that need to be manually reviewed below\n",
    "                { '$limit': 500 }\n",
    "            ])\n",
    "        ]\n",
    "        # Manual analaysis of the created dates revealed a number of broken or testing issues that should be ignored\n",
    "        if jira_name == 'Apache':\n",
    "            created_dates = created_dates[289:]\n",
    "        elif jira_name == 'Jira':\n",
    "            created_dates = created_dates[1:]\n",
    "        elif jira_name == 'IntelDAOS':\n",
    "            created_dates = created_dates[1:]\n",
    "        elif jira_name == 'Qt':\n",
    "            created_dates = created_dates[7:]\n",
    "        # Return value\n",
    "        return created_dates[0][:4]\n",
    "    \n",
    "    def extract_number_of_changes(jira_name):\n",
    "        # Query for the number of changes\n",
    "        query_result = list(db[jira_name].aggregate([\n",
    "            # We only need one attribute of the change to count it\n",
    "            { '$project': { '_id': 0, 'changelog.histories.items.field': 1 } },\n",
    "            # Unwind the histories and items arrays into single elements so we can count them\n",
    "            { '$unwind': '$changelog.histories' },\n",
    "            { '$unwind': '$changelog.histories.items' },\n",
    "            # Count number of elements in our aggregation, which is now the number of items\n",
    "            { '$count': 'num_changes' }\n",
    "        ]))\n",
    "        # Extract the query result and return\n",
    "        return query_result[0]['num_changes'] if query_result else 0\n",
    "    \n",
    "    def extract_number_of_unique_projects(jira_name):\n",
    "        # Query for a unique set of project ids in the final state of the issue\n",
    "        query_result = list(db[jira_name].aggregate([\n",
    "            # Limit to just the final project name on each issue\n",
    "            { '$project': { '_id': 0, 'project_name': '$fields.project.name' } },\n",
    "            # Create a unique set of project names across the entire Jira\n",
    "            { '$group': { '_id': None, 'project_names': { '$addToSet': '$project_name' } } }\n",
    "        ]))\n",
    "        # Extract the query result\n",
    "        unique_projects_final = set(query_result[0]['project_names']) if query_result else set()\n",
    "        # Query for a unique set of project ids in the issue history\n",
    "        query_result = list(db[jira_name].aggregate([\n",
    "            # Unwind the histories and items to work with individual change items\n",
    "            { '$unwind': '$changelog.histories' },\n",
    "            { '$unwind': '$changelog.histories.items' },\n",
    "            # Select only changes where the project field was changed\n",
    "            { '$match': { 'changelog.histories.items.field': 'project' } },\n",
    "            # Rename the nested 'fromString' field containing the previously slected project\n",
    "            { '$project': { '_id': 0, 'project_name': '$changelog.histories.items.fromString' } },\n",
    "            # Create a unique set of these project names\n",
    "            { '$group': { '_id': None, 'project_names': { '$addToSet': '$project_name' } } }\n",
    "        ]))\n",
    "        # Extract the query result\n",
    "        unique_projects_history = set(query_result[0]['project_names']) if query_result else set()\n",
    "        # Union the two sets together, count the items, and return\n",
    "        return len(set.union(unique_projects_final, unique_projects_history))\n",
    "    \n",
    "    def extract_number_of_comments(jira_name):\n",
    "        # Query for the number of changes\n",
    "        query_result = list(db[jira_name].aggregate([\n",
    "            # Get issues with comments\n",
    "            { '$match': { 'fields.comments': { '$ne': None } } },\n",
    "            # We only need one attribute of the change to count it\n",
    "            { '$project': { '_id': 0, 'num_comments_per_issue': { '$size': '$fields.comments' } } },\n",
    "            # Group the sizes so we can sum all to a single value for the repo\n",
    "            { '$group': { '_id': None, 'num_comments': { '$sum': '$num_comments_per_issue' } } },\n",
    "        ]))\n",
    "        # Extract the query result and return\n",
    "        return query_result[0]['num_comments'] if query_result else 0\n",
    "    \n",
    "    print('This script takes ~90 minutes when executed across all Jiras.')\n",
    "    \n",
    "    # Populate the table with the answers to our questions\n",
    "    for jira_name in jiras:\n",
    "        print(f\"\\tWorking on Jira: {jira_name} ...\")\n",
    "        \n",
    "        ## Issues and their Types ##\n",
    "        \n",
    "        # Attribute: Issues (number of issues)\n",
    "        df_jiras.loc[jira_name, 'Issues'] = extract_number_of_issues(jira_name)\n",
    "        # Attribute: DIT (documented issue types)\n",
    "        df_jiras.loc[jira_name, 'DIT'] = extract_number_of_documented_issuetypes(jira_name)\n",
    "        # Attribute: UIT (used issue types)\n",
    "        df_jiras.loc[jira_name, 'UIT'] = extract_number_of_used_issuetypes(jira_name)\n",
    "        \n",
    "        ## Issue Links and their Types ##\n",
    "\n",
    "        # Attribute: Links (number of links)\n",
    "        df_jiras.loc[jira_name, 'Links'] = extract_number_of_issuelinks(jira_name)\n",
    "        # Attribute: DLT (documented link types)\n",
    "        df_jiras.loc[jira_name, 'DLT'] = extract_number_of_documented_issuelinktypes(jira_name)\n",
    "        # Attribute: ULD (used link types)\n",
    "        df_jiras.loc[jira_name, 'ULT'] = extract_number_of_used_issuelinktypes(jira_name)\n",
    "        \n",
    "        ## General Information ##\n",
    "        \n",
    "        # Attribute: Born (first issue added)\n",
    "        df_jiras.loc[jira_name, 'Born'] = extract_born(jira_name)\n",
    "        # Attribute: Changes (number of changes)\n",
    "        df_jiras.loc[jira_name, 'Changes'] = extract_number_of_changes(jira_name)\n",
    "        # Attribute: Ch/I (number of changes per issue)\n",
    "        df_jiras.loc[jira_name, 'Ch/I'] = round(df_jiras.loc[jira_name, 'Changes'] / df_jiras.loc[jira_name, 'Issues'])\n",
    "        # Attribute: UP (unique projects)\n",
    "        df_jiras.loc[jira_name, 'UP'] = extract_number_of_unique_projects(jira_name)\n",
    "        # Attribute: Comments (number of comments)\n",
    "        df_jiras.loc[jira_name, 'Comments'] = extract_number_of_comments(jira_name)\n",
    "        # Attribute: Co/I (number of comments per issue)\n",
    "        df_jiras.loc[jira_name, 'Co/I'] = round(df_jiras.loc[jira_name, 'Comments'] / df_jiras.loc[jira_name, 'Issues'])\n",
    "        \n",
    "        \n",
    "    print('Complete')\n",
    "    return df_jiras\n",
    "        \n",
    "df_jiras = populate_df_jiras(\n",
    "    df_jiras,\n",
    "    ## Test to see if the script works (database created, data inside, etc.) ##\n",
    "    # jiras=['Hyperledger'],\n",
    "    ## To test the script in less than 90 minutes, uncomment the following line and see the result of a few select Jira repos ##\n",
    "    # jiras=['Hyperledger', 'IntelDAOS', 'JFrog', 'Sakai', 'SecondLife', 'Sonatype', 'Spring'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30953763",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9599b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dtale\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def explore_jiras_in_dtale(selected_jiras=None):\n",
    "    \"\"\"\n",
    "    Connects to 'JiraRepos' MongoDB, iterates through Jira collections,\n",
    "    creates a DataFrame for each, and opens a D-Tale session so you can\n",
    "    explore all columns interactively.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    selected_jiras : list of str, optional\n",
    "        If provided, only these Jira repos will be loaded. Otherwise,\n",
    "        all Jiras from the config file will be loaded.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Connect to MongoDB\n",
    "    client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "    db = client[\"JiraRepos\"]\n",
    "    \n",
    "    # 2. Load your JIRA data sources config\n",
    "    with open(\"../0. DataDefinition/jira_data_sources.json\") as f:\n",
    "        jira_data_sources = json.load(f)\n",
    "    \n",
    "    # 3. Determine all available JIRA names from the config\n",
    "    all_jiras = list(jira_data_sources.keys())\n",
    "\n",
    "    # If specific repos were requested, filter them\n",
    "    if selected_jiras is not None and len(selected_jiras) > 0:\n",
    "        # Only keep repos that exist in the config\n",
    "        all_jiras = [j for j in all_jiras if j in selected_jiras]\n",
    "        if not all_jiras:\n",
    "            print(f\"⚠️ No valid Jiras found in {selected_jiras}. Check your input.\")\n",
    "            return\n",
    "\n",
    "    # 4. For each JIRA collection, create a DataFrame & open D-Tale\n",
    "    for jira_name in all_jiras:\n",
    "        print(f\"Loading issues from collection: {jira_name} ...\")\n",
    "        issues = list(db[jira_name].find())\n",
    "        \n",
    "        if not issues:\n",
    "            print(f\"⚠️ No documents found for '{jira_name}', skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Create a raw DataFrame with all columns (all fields from each document)\n",
    "        df = pd.DataFrame(issues)\n",
    "        \n",
    "        # Show in D-Tale\n",
    "        print(f\"Opening D-Tale for {jira_name}... (close browser tab to move on)\")\n",
    "        d = dtale.show(\n",
    "            df,\n",
    "            ignore_duplicate=True,   # avoid errors if run multiple times\n",
    "            allow_cell_edits=False   # optional, disallow edits if you prefer\n",
    "        )\n",
    "        # Optionally open a browser tab automatically\n",
    "        d.open_browser()\n",
    "        \n",
    "        # If you want to block execution until you close D-Tale, you can do:\n",
    "        # d.wait_for_close()\n",
    "        # Otherwise, the script moves on to next Jira.\n",
    "\n",
    "    print(\"✅ D-Tale sessions launched for the selected Jira DataFrames.\")\n",
    "\n",
    "# --- If you want to run this script directly, test with some custom repos ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage #1: Use ALL repos defined in the config\n",
    "    explore_jiras_in_dtale()\n",
    "\n",
    "    # Example usage #2: Only visualize 'Hyperledger' and 'SecondLife' \n",
    "    # (comment out the first call if you want only the second)\n",
    "    # explore_jiras_in_dtale([\"Hyperledger\", \"SecondLife\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c7ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba659c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dtale\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def explore_all_fields_in_dtale(selected_jiras=None):\n",
    "    \"\"\"\n",
    "    Connects to 'JiraRepos' in MongoDB, iterates through JIRA collections,\n",
    "    flattens each document (including nested fields), creates a DataFrame,\n",
    "    and opens a D-Tale session so you can explore all columns interactively.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    selected_jiras : list of str, optional\n",
    "        If provided, only these Jira repos will be loaded. Otherwise,\n",
    "        all Jiras from the config file will be loaded.\n",
    "    \"\"\"\n",
    "    # 1. Connect to MongoDB\n",
    "    client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "    db = client[\"JiraRepos\"]\n",
    "    \n",
    "    # 2. Load your JIRA data sources config\n",
    "    with open(\"../0. DataDefinition/jira_data_sources.json\") as f:\n",
    "        jira_data_sources = json.load(f)\n",
    "    \n",
    "    # 3. Determine all available JIRA names from the config\n",
    "    all_jiras = list(jira_data_sources.keys())\n",
    "    \n",
    "    # Filter if specific repos were requested\n",
    "    if selected_jiras is not None and len(selected_jiras) > 0:\n",
    "        all_jiras = [j for j in all_jiras if j in selected_jiras]\n",
    "        if not all_jiras:\n",
    "            print(f\"⚠️ No valid Jiras found in {selected_jiras}. Check your input.\")\n",
    "            return\n",
    "    \n",
    "    # 4. For each JIRA collection, load, flatten, and display in D-Tale\n",
    "    for jira_name in all_jiras:\n",
    "        print(f\"Loading issues from collection: {jira_name} ...\")\n",
    "        issues = list(db[jira_name].find())\n",
    "        \n",
    "        if not issues:\n",
    "            print(f\"⚠️ No documents found for '{jira_name}', skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Flatten nested JSON using pd.json_normalize.\n",
    "        # You can adjust the separator if desired (default is '.')\n",
    "        df = pd.json_normalize(issues, sep='.')\n",
    "        \n",
    "        # Show in D-Tale\n",
    "        print(f\"Opening D-Tale for {jira_name}... (close browser tab to move on)\")\n",
    "        d = dtale.show(\n",
    "            df,\n",
    "            ignore_duplicate=True,   # avoid errors if run multiple times\n",
    "            allow_cell_edits=False   # optional, disallow edits if you prefer\n",
    "        )\n",
    "        d.open_browser()\n",
    "        \n",
    "    print(\"✅ D-Tale sessions launched for the selected Jira DataFrames.\")\n",
    "\n",
    "# --- If you want to run this script directly, test with some custom repos ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage: Load ALL repos defined in the config\n",
    "    # explore_all_fields_in_dtale()\n",
    "\n",
    "    # Example usage: Only visualize 'Hyperledger' and 'SecondLife'\n",
    "    explore_all_fields_in_dtale([\"Hyperledger\", \"SecondLife\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35839635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dtale\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def flatten_histories(histories):\n",
    "    \"\"\"\n",
    "    Given a list of changelog history entries, returns a DataFrame\n",
    "    where each row corresponds to one change item.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for history in histories:\n",
    "        history_id = history.get(\"id\")\n",
    "        # Choose either 'name' or 'displayName' from the author\n",
    "        author = history.get(\"author\", {}).get(\"name\")\n",
    "        created = history.get(\"created\")\n",
    "        items = history.get(\"items\", [])\n",
    "        for item in items:\n",
    "            rows.append({\n",
    "                \"changelog.history_id\": history_id,\n",
    "                \"changelog.author\": author,\n",
    "                \"changelog.created\": created,\n",
    "                \"changelog.field\": item.get(\"field\"),\n",
    "                \"changelog.fieldtype\": item.get(\"fieldtype\"),\n",
    "                \"changelog.from\": item.get(\"from\"),\n",
    "                \"changelog.fromString\": item.get(\"fromString\"),\n",
    "                \"changelog.to\": item.get(\"to\"),\n",
    "                \"changelog.toString\": item.get(\"toString\")\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def process_issue_histories(issue):\n",
    "    \"\"\"\n",
    "    Process one issue: if it has a changelog with histories,\n",
    "    flatten them and attach the issue identifier.\n",
    "    \"\"\"\n",
    "    if \"changelog\" in issue and \"histories\" in issue[\"changelog\"]:\n",
    "        histories = issue[\"changelog\"][\"histories\"]\n",
    "        df_history = flatten_histories(histories)\n",
    "        # Add an identifier for the issue (using 'key' if available, else 'id')\n",
    "        df_history[\"issue_key\"] = issue.get(\"key\", issue.get(\"id\"))\n",
    "        return df_history\n",
    "    return None\n",
    "\n",
    "def extract_and_flatten_histories(issues):\n",
    "    \"\"\"\n",
    "    Extracts and flattens changelog.histories from a list of issues using parallel processing.\n",
    "    Uses a ThreadPoolExecutor for compatibility in notebook environments.\n",
    "    \"\"\"\n",
    "    flattened_histories = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_issue_histories, issue): issue for issue in issues}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None and not result.empty:\n",
    "                flattened_histories.append(result)\n",
    "    if flattened_histories:\n",
    "        return pd.concat(flattened_histories, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def explore_all_fields_in_dtale(selected_jiras=None):\n",
    "    \"\"\"\n",
    "    Connects to the MongoDB 'JiraRepos' database, loads issues from the specified\n",
    "    Jira collections, and:\n",
    "      - Flattens the main issue documents using pd.json_normalize.\n",
    "      - Extracts and flattens the changelog.histories field from each issue using parallel processing.\n",
    "      - Aggregates the flattened histories (grouped by issue) and merges them as new columns into the main DataFrame.\n",
    "      - Opens a single D-Tale session with the merged data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    selected_jiras : list of str, optional\n",
    "        If provided, only these Jira repos will be loaded. Otherwise, all repos in the config are used.\n",
    "    \"\"\"\n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "    db = client[\"JiraRepos\"]\n",
    "    \n",
    "    # Load JIRA data sources config\n",
    "    with open(\"../0. DataDefinition/jira_data_sources.json\") as f:\n",
    "        jira_data_sources = json.load(f)\n",
    "    \n",
    "    # Determine available JIRA names from the config\n",
    "    all_jiras = list(jira_data_sources.keys())\n",
    "    if selected_jiras is not None and len(selected_jiras) > 0:\n",
    "        all_jiras = [j for j in all_jiras if j in selected_jiras]\n",
    "        if not all_jiras:\n",
    "            print(f\"⚠️ No valid Jiras found in {selected_jiras}.\")\n",
    "            return\n",
    "    \n",
    "    merged_dfs = []  # To store merged DataFrames from each repo\n",
    "    for jira_name in all_jiras:\n",
    "        print(f\"\\nLoading issues from collection: {jira_name} ...\")\n",
    "        issues = list(db[jira_name].find())\n",
    "        if not issues:\n",
    "            print(f\"⚠️ No documents found for '{jira_name}', skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Flatten the main issue structure using pd.json_normalize\n",
    "        df_main = pd.json_normalize(issues, sep='.')\n",
    "        \n",
    "        # Extract and flatten changelog.histories using parallel processing\n",
    "        df_histories = extract_and_flatten_histories(issues)\n",
    "        \n",
    "        if not df_histories.empty:\n",
    "            # Aggregate histories by issue: group by 'issue_key' and collect each column's values as a list.\n",
    "            agg_histories = df_histories.groupby(\"issue_key\").agg(lambda x: list(x)).reset_index()\n",
    "            # Ensure df_main has a column that can join with 'issue_key'. Use 'key' if available, else 'id'.\n",
    "            if \"key\" not in df_main.columns:\n",
    "                df_main[\"key\"] = df_main[\"id\"]\n",
    "            # Merge the aggregated histories into the main dataframe.\n",
    "            df_merged = pd.merge(df_main, agg_histories, how=\"left\", left_on=\"key\", right_on=\"issue_key\")\n",
    "            # Optionally, drop the redundant 'issue_key' column.\n",
    "            df_merged.drop(columns=[\"issue_key\"], inplace=True)\n",
    "        else:\n",
    "            df_merged = df_main\n",
    "        \n",
    "        merged_dfs.append(df_merged)\n",
    "    \n",
    "    # Combine all repos into one DataFrame if more than one was processed.\n",
    "    if merged_dfs:\n",
    "        final_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No data to display.\")\n",
    "        return\n",
    "    \n",
    "    # Open a single D-Tale session for the merged DataFrame\n",
    "    print(\"Opening D-Tale for merged issues data (including aggregated changelog.histories)...\")\n",
    "    d = dtale.show(final_df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "    d.open_browser()\n",
    "    \n",
    "    print(\"✅ D-Tale session launched for the merged Jira DataFrame.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage: visualize specific Jira repos\n",
    "    explore_all_fields_in_dtale([\"Hyperledger\", \"SecondLife\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38276cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading issues from collection: Hyperledger ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 172\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ D-Tale session launched for the sampled Jira DataFrame.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# Example usage: visualize specific Jira repos, loading only 20% of their issues\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m     \u001b[43mexplore_all_fields_in_dtale\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHyperledger\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSecondLife\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 116\u001b[0m, in \u001b[0;36mexplore_all_fields_in_dtale\u001b[0;34m(selected_jiras, sample_ratio)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m jira_name \u001b[38;5;129;01min\u001b[39;00m all_jiras:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLoading issues from collection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjira_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m     issues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mjira_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issues:\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ No documents found for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjira_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, skipping.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pymongo/synchronous/cursor.py:1281\u001b[0m, in \u001b[0;36mCursor.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _DocumentType:\n\u001b[0;32m-> 1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pymongo/synchronous/cursor.py:1257\u001b[0m, in \u001b[0;36mCursor.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_empty:\n\u001b[1;32m   1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m-> 1257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_refresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pymongo/synchronous/cursor.py:1228\u001b[0m, in \u001b[0;36mCursor._refresh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;66;03m# Exhaust cursors don't send getMore messages.\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getmore_class(\n\u001b[1;32m   1215\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbname,\n\u001b[1;32m   1216\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_comment,\n\u001b[1;32m   1227\u001b[0m     )\n\u001b[0;32m-> 1228\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pymongo/synchronous/cursor.py:1100\u001b[0m, in \u001b[0;36mCursor._send_message\u001b[0;34m(self, operation)\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidOperation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexhaust cursors do not support auto encryption\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_operation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unpack_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_address\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OperationFailure \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01min\u001b[39;00m _CURSOR_CLOSED_ERRORS \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exhaust:\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;66;03m# Don't send killCursors because the cursor is already closed.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pymongo/_csot.py:119\u001b[0m, in \u001b[0;36mapply.<locals>.csot_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[1;32m    118\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pymongo/synchronous/mongo_client.py:1752\u001b[0m, in \u001b[0;36mMongoClient._run_operation\u001b[0;34m(self, operation, unpack_res, address)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     operation\u001b[38;5;241m.\u001b[39mreset()  \u001b[38;5;66;03m# Reset op in case of retry.\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m server\u001b[38;5;241m.\u001b[39mrun_operation(\n\u001b[1;32m   1744\u001b[0m         conn,\n\u001b[1;32m   1745\u001b[0m         operation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1750\u001b[0m     )\n\u001b[0;32m-> 1752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retryable_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_cmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretryable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Query\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pymongo/synchronous/mongo_client.py:1861\u001b[0m, in \u001b[0;36mMongoClient._retryable_read\u001b[0;34m(self, func, read_pref, session, operation, address, retryable, operation_id)\u001b[0m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;66;03m# Ensure that the client supports retrying on reads and there is no session in\u001b[39;00m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;66;03m# transaction, otherwise, we will not support retry behavior for this call.\u001b[39;00m\n\u001b[1;32m   1858\u001b[0m retryable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(\n\u001b[1;32m   1859\u001b[0m     retryable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mretry_reads \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (session \u001b[38;5;129;01mand\u001b[39;00m session\u001b[38;5;241m.\u001b[39min_transaction)\n\u001b[1;32m   1860\u001b[0m )\n\u001b[0;32m-> 1861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_read\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1867\u001b[0m \u001b[43m    \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretryable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pymongo/_csot.py:119\u001b[0m, in \u001b[0;36mapply.<locals>.csot_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[1;32m    118\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pymongo/synchronous/mongo_client.py:1817\u001b[0m, in \u001b[0;36mMongoClient._retry_internal\u001b[0;34m(self, func, session, bulk, operation, is_read, address, read_pref, retryable, operation_id)\u001b[0m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;129m@_csot\u001b[39m\u001b[38;5;241m.\u001b[39mapply\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_retry_internal\u001b[39m(\n\u001b[1;32m   1793\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1802\u001b[0m     operation_id: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1803\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Internal retryable helper for all client transactions.\u001b[39;00m\n\u001b[1;32m   1805\u001b[0m \n\u001b[1;32m   1806\u001b[0m \u001b[38;5;124;03m    :param func: Callback function we want to retry\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;124;03m    :return: Output of the calling func()\u001b[39;00m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ClientConnectionRetryable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmongo_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1819\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbulk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbulk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1821\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_read\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_read\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1823\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1825\u001b[0m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretryable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1827\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1828\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pymongo/synchronous/mongo_client.py:2565\u001b[0m, in \u001b[0;36m_ClientConnectionRetryable.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_last_error(check_csot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_read \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write()\n\u001b[1;32m   2566\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ServerSelectionTimeoutError:\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;66;03m# The application may think the write was never attempted\u001b[39;00m\n\u001b[1;32m   2568\u001b[0m     \u001b[38;5;66;03m# if we raise ServerSelectionTimeoutError on the retry\u001b[39;00m\n\u001b[1;32m   2569\u001b[0m     \u001b[38;5;66;03m# attempt. Raise the original exception instead.\u001b[39;00m\n\u001b[1;32m   2570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_last_error()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pymongo/synchronous/mongo_client.py:2708\u001b[0m, in \u001b[0;36m_ClientConnectionRetryable._read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrying \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retryable:\n\u001b[1;32m   2707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_last_error()\n\u001b[0;32m-> 2708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_server\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pymongo/synchronous/mongo_client.py:1743\u001b[0m, in \u001b[0;36mMongoClient._run_operation.<locals>._cmd\u001b[0;34m(_session, server, conn, read_preference)\u001b[0m\n\u001b[1;32m   1736\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_cmd\u001b[39m(\n\u001b[1;32m   1737\u001b[0m     _session: Optional[ClientSession],\n\u001b[1;32m   1738\u001b[0m     server: Server,\n\u001b[1;32m   1739\u001b[0m     conn: Connection,\n\u001b[1;32m   1740\u001b[0m     read_preference: _ServerMode,\n\u001b[1;32m   1741\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m   1742\u001b[0m     operation\u001b[38;5;241m.\u001b[39mreset()  \u001b[38;5;66;03m# Reset op in case of retry.\u001b[39;00m\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_operation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_listeners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m        \u001b[49m\u001b[43munpack_res\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pymongo/synchronous/helpers.py:47\u001b[0m, in \u001b[0;36m_handle_reauth.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpymongo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msynchronous\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Connection\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OperationFailure \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_reauth:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pymongo/synchronous/server.py:208\u001b[0m, in \u001b[0;36mServer.run_operation\u001b[0;34m(self, conn, operation, read_preference, listeners, unpack_res, client)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     conn\u001b[38;5;241m.\u001b[39msend_message(data, max_doc_size)\n\u001b[0;32m--> 208\u001b[0m     reply \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Unpack and check for command errors.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cmd:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pymongo/synchronous/pool.py:590\u001b[0m, in \u001b[0;36mConnection.receive_message\u001b[0;34m(self, request_id)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m receive_message(\u001b[38;5;28mself\u001b[39m, request_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_message_size)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m--> 590\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_connection_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pymongo/synchronous/pool.py:588\u001b[0m, in \u001b[0;36mConnection.receive_message\u001b[0;34m(self, request_id)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Receive a raw BSON message or raise ConnectionFailure.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \n\u001b[1;32m    585\u001b[0m \u001b[38;5;124;03mIf any exception is raised, the socket is closed.\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreceive_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_message_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_connection_failure(error)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pymongo/synchronous/network.py:333\u001b[0m, in \u001b[0;36mreceive_message\u001b[0;34m(conn, request_id, max_message_size)\u001b[0m\n\u001b[1;32m    331\u001b[0m     data \u001b[38;5;241m=\u001b[39m decompress(receive_data(conn, length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m25\u001b[39m, deadline), compressor_id)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 333\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mreceive_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     unpack_reply \u001b[38;5;241m=\u001b[39m _UNPACK_REPLY[op_code]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pymongo/network_layer.py:389\u001b[0m, in \u001b[0;36mreceive_data\u001b[0;34m(conn, length, deadline)\u001b[0m\n\u001b[1;32m    386\u001b[0m             short_timeout \u001b[38;5;241m=\u001b[39m _POLL_TIMEOUT\n\u001b[1;32m    387\u001b[0m         conn\u001b[38;5;241m.\u001b[39mset_conn_timeout(short_timeout)\n\u001b[0;32m--> 389\u001b[0m     chunk_length \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbytes_read\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BLOCKING_IO_ERRORS:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mcancel_context\u001b[38;5;241m.\u001b[39mcancelled:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import dtale\n",
    "import pandas as pd\n",
    "import random\n",
    "from pymongo import MongoClient\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import dateparser\n",
    "\n",
    "def convert_date_columns_dateparser(df, date_columns):\n",
    "    \"\"\"\n",
    "    Convert the specified date columns from string to datetime using dateparser.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input DataFrame containing date strings.\n",
    "      date_columns (list): List of column names to convert.\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: The DataFrame with specified columns converted to datetime objects.\n",
    "    \"\"\"\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(lambda x: dateparser.parse(x, \n",
    "                                    settings={'RETURN_AS_TIMEZONE_AWARE': True, 'TIMEZONE': 'UTC'})\n",
    "                                    if pd.notnull(x) else pd.NaT)\n",
    "    return df\n",
    "\n",
    "def flatten_histories(histories):\n",
    "    \"\"\"\n",
    "    Given a list of changelog history entries, returns a DataFrame\n",
    "    where each row corresponds to one change item.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for history in histories:\n",
    "        history_id = history.get(\"id\")\n",
    "        author = history.get(\"author\", {}).get(\"name\")\n",
    "        created = history.get(\"created\")\n",
    "        items = history.get(\"items\", [])\n",
    "\n",
    "        for item in items:\n",
    "            rows.append({\n",
    "                \"changelog.history_id\": history_id,\n",
    "                \"changelog.author\": author,\n",
    "                \"changelog.created\": created,\n",
    "                \"changelog.field\": item.get(\"field\"),\n",
    "                \"changelog.fieldtype\": item.get(\"fieldtype\"),\n",
    "                \"changelog.from\": item.get(\"from\"),\n",
    "                \"changelog.fromString\": item.get(\"fromString\"),\n",
    "                \"changelog.to\": item.get(\"to\"),\n",
    "                \"changelog.toString\": item.get(\"toString\")\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def process_issue_histories(issue):\n",
    "    \"\"\"\n",
    "    Process one issue: If it has a changelog with histories,\n",
    "    flatten them and attach the issue identifier.\n",
    "    \"\"\"\n",
    "    if \"changelog\" in issue and \"histories\" in issue[\"changelog\"]:\n",
    "        histories = issue[\"changelog\"][\"histories\"]\n",
    "        df_history = flatten_histories(histories)\n",
    "        df_history[\"issue_key\"] = issue.get(\"key\", issue.get(\"id\"))\n",
    "        return df_history\n",
    "    return None\n",
    "\n",
    "def extract_and_flatten_histories(issues):\n",
    "    \"\"\"\n",
    "    Extracts and flattens changelog.histories from a list of issues using parallel processing.\n",
    "    Uses a ThreadPoolExecutor for better performance.\n",
    "    \"\"\"\n",
    "    flattened_histories = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_issue_histories, issue): issue for issue in issues}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None and not result.empty:\n",
    "                flattened_histories.append(result)\n",
    "\n",
    "    if flattened_histories:\n",
    "        return pd.concat(flattened_histories, ignore_index=True)\n",
    "    \n",
    "    return pd.DataFrame()\n",
    "\n",
    "def explore_all_fields_in_dtale(selected_jiras=None, sample_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Connects to the MongoDB 'JiraRepos' database, loads only 20% of issues from each repository,\n",
    "    flattens changelog histories, and launches a D-Tale session.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    selected_jiras : list of str, optional\n",
    "        If provided, only these Jira repos will be loaded. Otherwise, all repos in the config are used.\n",
    "    sample_ratio : float, optional\n",
    "        The fraction of issues to load per repo (default is 20%).\n",
    "    \"\"\"\n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "    db = client[\"JiraRepos\"]\n",
    "    \n",
    "    # Load JIRA data sources config\n",
    "    with open(\"../0. DataDefinition/jira_data_sources.json\") as f:\n",
    "        jira_data_sources = json.load(f)\n",
    "\n",
    "    # Determine available JIRA names from the config\n",
    "    all_jiras = list(jira_data_sources.keys())\n",
    "    if selected_jiras is not None and len(selected_jiras) > 0:\n",
    "        all_jiras = [j for j in all_jiras if j in selected_jiras]\n",
    "        if not all_jiras:\n",
    "            print(f\"⚠️ No valid Jiras found in {selected_jiras}.\")\n",
    "            return\n",
    "\n",
    "    merged_dfs = []  # To store processed DataFrames\n",
    "\n",
    "    for jira_name in all_jiras:\n",
    "        print(f\"\\nLoading issues from collection: {jira_name} ...\")\n",
    "        issues = list(db[jira_name].find())\n",
    "\n",
    "        if not issues:\n",
    "            print(f\"⚠️ No documents found for '{jira_name}', skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Randomly sample 20% of issues\n",
    "        sample_size = max(1, int(len(issues) * sample_ratio))  # At least 1 issue per repo\n",
    "        sampled_issues = random.sample(issues, sample_size)\n",
    "\n",
    "        \n",
    "\n",
    "        # Flatten the main issue structure using pd.json_normalize\n",
    "        df_main = pd.json_normalize(sampled_issues, sep='.')\n",
    "\n",
    "        # Extract and flatten changelog.histories\n",
    "        df_histories = extract_and_flatten_histories(sampled_issues)\n",
    "\n",
    "        if not df_histories.empty:\n",
    "            # Aggregate histories by issue: group by 'issue_key' and collect each column's values as a list.\n",
    "            agg_histories = df_histories.groupby(\"issue_key\").agg(lambda x: list(x)).reset_index()\n",
    "\n",
    "            # Ensure df_main has a column that can join with 'issue_key'. Use 'key' if available, else 'id'.\n",
    "            if \"key\" not in df_main.columns:\n",
    "                df_main[\"key\"] = df_main[\"id\"]\n",
    "\n",
    "            # Merge the aggregated histories into the main dataframe.\n",
    "            df_merged = pd.merge(df_main, agg_histories, how=\"left\", left_on=\"key\", right_on=\"issue_key\")\n",
    "            df_merged.drop(columns=[\"issue_key\"], inplace=True, errors='ignore')\n",
    "        else:\n",
    "            df_merged = df_main\n",
    "\n",
    "        merged_dfs.append(df_merged)\n",
    "\n",
    "    # Combine all sampled repos into one DataFrame\n",
    "    if merged_dfs:\n",
    "        final_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No data to display.\")\n",
    "        return\n",
    "    \n",
    "    # Convert known date columns using dateparser\n",
    "    date_columns = [\"fields.created\", \"fields.updated\", \"fields.resolutiondate\", \"changelog.created\"]\n",
    "    final_df = convert_date_columns_dateparser(final_df, date_columns)\n",
    "    \n",
    "     # Convert known date columns using dateparser\n",
    "    print(\"Converting date columns to date objects\")\n",
    "    date_columns = [\"fields.created\", \"fields.updated\", \"fields.resolutiondate\", \"changelog.created\"]\n",
    "    final_df = convert_date_columns_dateparser(final_df, date_columns)\n",
    "    \n",
    "    \n",
    "    # Open D-Tale for visualization\n",
    "    print(\"Opening D-Tale for sampled issues data (including aggregated changelog.histories)...\")\n",
    "    d = dtale.show(final_df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "    d.open_browser()\n",
    "    \n",
    "    print(\"✅ D-Tale session launched for the sampled Jira DataFrame.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage: visualize specific Jira repos, loading only 20% of their issues\n",
    "    explore_all_fields_in_dtale([\"Hyperledger\", \"SecondLife\"], sample_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffcb82d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c38ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading issues from collection: Hyperledger ...\n",
      "\n",
      "Loading issues from collection: SecondLife ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_21492/977894954.py:158: FutureWarning:\n",
      "\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n",
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_21492/977894954.py:158: FutureWarning:\n",
      "\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types have been fixed for flattened data. Launching D-Tale session...\n",
      "✅ D-Tale session launched for the processed Jira DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/column_builders.py:519: UserWarning:\n",
      "\n",
      "The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/column_builders.py:519: FutureWarning:\n",
      "\n",
      "In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import dtale\n",
    "import pandas as pd\n",
    "import random\n",
    "from pymongo import MongoClient\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import dateparser\n",
    "\n",
    "def fix_data_types(df, numeric_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Converts DataFrame columns stored as strings into proper data types.\n",
    "    For each column:\n",
    "      - If at least `numeric_threshold` of values can be converted to numeric, convert the column.\n",
    "      - Otherwise, cast the column to a 'category' dtype.\n",
    "    Columns containing list-like entries (from aggregation) are skipped.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame.\n",
    "    numeric_threshold : float, optional\n",
    "        The fraction of values that must be numeric for conversion (default 0.9).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with updated types.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        # Skip columns with list-like entries (e.g., aggregated changelog data)\n",
    "        if df[col].apply(lambda x: isinstance(x, list)).any():\n",
    "            continue\n",
    "        numeric_series = pd.to_numeric(df[col], errors='coerce')\n",
    "        non_null_ratio = numeric_series.notnull().mean()\n",
    "        if non_null_ratio >= numeric_threshold:\n",
    "            df[col] = numeric_series\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "def flatten_histories(histories):\n",
    "    \"\"\"\n",
    "    Given a list of changelog history entries, returns a DataFrame where each row corresponds to one change item.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for history in histories:\n",
    "        history_id = history.get(\"id\")\n",
    "        author = history.get(\"author\", {}).get(\"name\")\n",
    "        created = history.get(\"created\")\n",
    "        items = history.get(\"items\", [])\n",
    "        for item in items:\n",
    "            rows.append({\n",
    "                \"changelog.history_id\": history_id,\n",
    "                \"changelog.author\": author,\n",
    "                \"changelog.created\": created,\n",
    "                \"changelog.field\": item.get(\"field\"),\n",
    "                \"changelog.fieldtype\": item.get(\"fieldtype\"),\n",
    "                \"changelog.from\": item.get(\"from\"),\n",
    "                \"changelog.fromString\": item.get(\"fromString\"),\n",
    "                \"changelog.to\": item.get(\"to\"),\n",
    "                \"changelog.toString\": item.get(\"toString\")\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def process_issue_histories(issue):\n",
    "    \"\"\"\n",
    "    Process one issue by flattening its changelog histories.\n",
    "    After flattening, the DataFrame is converted to proper data types.\n",
    "    \"\"\"\n",
    "    if \"changelog\" in issue and \"histories\" in issue[\"changelog\"]:\n",
    "        histories = issue[\"changelog\"][\"histories\"]\n",
    "        df_history = flatten_histories(histories)\n",
    "        # Convert types right after flattening histories\n",
    "        df_history = fix_data_types(df_history)\n",
    "        df_history[\"issue_key\"] = issue.get(\"key\", issue.get(\"id\"))\n",
    "        return df_history\n",
    "    return None\n",
    "\n",
    "def extract_and_flatten_histories(issues):\n",
    "    \"\"\"\n",
    "    Extracts and flattens changelog.histories from a list of issues using parallel processing.\n",
    "    \"\"\"\n",
    "    flattened_histories = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_issue_histories, issue): issue for issue in issues}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None and not result.empty:\n",
    "                flattened_histories.append(result)\n",
    "    if flattened_histories:\n",
    "        return pd.concat(flattened_histories, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def explore_all_fields_in_dtale(selected_jiras=None, sample_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Connects to the MongoDB 'JiraRepos' database, samples issues from each repository,\n",
    "    flattens changelog histories with parallel processing, converts data types appropriately,\n",
    "    and launches a D-Tale session for interactive visualization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    selected_jiras : list of str, optional\n",
    "        If provided, only these Jira repos will be loaded.\n",
    "    sample_ratio : float, optional\n",
    "        The fraction of issues to load per repo (default is 0.2).\n",
    "    \"\"\"\n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "    db = client[\"JiraRepos\"]\n",
    "    \n",
    "    # Load JIRA data sources config\n",
    "    with open(\"../0. DataDefinition/jira_data_sources.json\") as f:\n",
    "        jira_data_sources = json.load(f)\n",
    "    \n",
    "    all_jiras = list(jira_data_sources.keys())\n",
    "    if selected_jiras is not None and len(selected_jiras) > 0:\n",
    "        all_jiras = [j for j in all_jiras if j in selected_jiras]\n",
    "        if not all_jiras:\n",
    "            print(f\"⚠️ No valid Jiras found in {selected_jiras}.\")\n",
    "            return\n",
    "    \n",
    "    merged_dfs = []  # To store processed DataFrames\n",
    "    \n",
    "    for jira_name in all_jiras:\n",
    "        print(f\"\\nLoading issues from collection: {jira_name} ...\")\n",
    "        issues = list(db[jira_name].find())\n",
    "        if not issues:\n",
    "            print(f\"⚠️ No documents found for '{jira_name}', skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Randomly sample a fraction of issues (ensuring at least one issue)\n",
    "        sample_size = max(1, int(len(issues) * sample_ratio))\n",
    "        sampled_issues = random.sample(issues, sample_size)\n",
    "        \n",
    "        # Flatten the main issue structure and convert data types immediately\n",
    "        df_main = pd.json_normalize(sampled_issues, sep='.')\n",
    "        df_main = fix_data_types(df_main)\n",
    "        \n",
    "        # Extract and flatten changelog.histories (each flattened DataFrame is already converted)\n",
    "        df_histories = extract_and_flatten_histories(sampled_issues)\n",
    "        \n",
    "        if not df_histories.empty:\n",
    "            # Aggregate histories by issue: group by 'issue_key' and collect values as lists.\n",
    "            agg_histories = df_histories.groupby(\"issue_key\").agg(lambda x: list(x)).reset_index()\n",
    "            \n",
    "            # Ensure df_main has a key to join on.\n",
    "            if \"key\" not in df_main.columns:\n",
    "                df_main[\"key\"] = df_main[\"id\"]\n",
    "            \n",
    "            # Merge aggregated histories into the main DataFrame.\n",
    "            df_merged = pd.merge(df_main, agg_histories, how=\"left\", left_on=\"key\", right_on=\"issue_key\")\n",
    "            df_merged.drop(columns=[\"issue_key\"], inplace=True, errors='ignore')\n",
    "        else:\n",
    "            df_merged = df_main\n",
    "        \n",
    "        merged_dfs.append(df_merged)\n",
    "    \n",
    "    if merged_dfs:\n",
    "        final_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No data to display.\")\n",
    "        return\n",
    "    \n",
    "    # At this point the main DataFrame has been fixed and aggregated data (lists) is left untouched.\n",
    "    print(\"Data types have been fixed for flattened data. Launching D-Tale session...\")\n",
    "    d = dtale.show(final_df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "    d.open_browser()\n",
    "    \n",
    "    print(\"✅ D-Tale session launched for the processed Jira DataFrame.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage: visualize specific Jira repos, loading only 20% of their issues.\n",
    "    explore_all_fields_in_dtale([\"Hyperledger\", \"SecondLife\"], sample_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59602ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dtale\n",
    "import pandas as pd\n",
    "import random\n",
    "from pymongo import MongoClient\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def fix_data_types(df, numeric_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Convert DataFrame columns (stored as strings) to appropriate data types,\n",
    "    excluding any date formatting.\n",
    "\n",
    "    For each column that does not contain list-like entries:\n",
    "      - If at least `numeric_threshold` fraction of values can be converted to numeric,\n",
    "        the column is converted to a numeric dtype.\n",
    "      - Otherwise, the column is cast to 'category' dtype.\n",
    "    \n",
    "    Note: Date-like strings remain as strings.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].apply(lambda x: isinstance(x, list)).any():\n",
    "            continue\n",
    "        numeric_series = pd.to_numeric(df[col], errors='coerce')\n",
    "        if numeric_series.notnull().mean() >= numeric_threshold:\n",
    "            df[col] = numeric_series\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "def flatten_histories(histories):\n",
    "    \"\"\"\n",
    "    Flatten a list of changelog history entries into a DataFrame.\n",
    "    Each row represents a single change item.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for history in histories:\n",
    "        history_id = history.get(\"id\")\n",
    "        author = history.get(\"author\", {}).get(\"name\")\n",
    "        created = history.get(\"created\")\n",
    "        items = history.get(\"items\", [])\n",
    "        for item in items:\n",
    "            rows.append({\n",
    "                \"changelog.history_id\": history_id,\n",
    "                \"changelog.author\": author,\n",
    "                \"changelog.created\": created,\n",
    "                \"changelog.field\": item.get(\"field\"),\n",
    "                \"changelog.fieldtype\": item.get(\"fieldtype\"),\n",
    "                \"changelog.from\": item.get(\"from\"),\n",
    "                \"changelog.fromString\": item.get(\"fromString\"),\n",
    "                \"changelog.to\": item.get(\"to\"),\n",
    "                \"changelog.toString\": item.get(\"toString\")\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def process_issue_histories(issue):\n",
    "    \"\"\"\n",
    "    Process a single issue's changelog histories by flattening them and applying type conversion.\n",
    "    Adds an 'issue_key' for later merging.\n",
    "    \"\"\"\n",
    "    if \"changelog\" in issue and \"histories\" in issue[\"changelog\"]:\n",
    "        histories = issue[\"changelog\"][\"histories\"]\n",
    "        df_history = flatten_histories(histories)\n",
    "        df_history = fix_data_types(df_history)\n",
    "        df_history[\"issue_key\"] = issue.get(\"key\", issue.get(\"id\"))\n",
    "        return df_history\n",
    "    return None\n",
    "\n",
    "def extract_and_flatten_histories(issues):\n",
    "    \"\"\"\n",
    "    Extract and flatten changelog histories from a list of issues using parallel processing.\n",
    "    \"\"\"\n",
    "    flattened_histories = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_issue_histories, issue): issue for issue in issues}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None and not result.empty:\n",
    "                flattened_histories.append(result)\n",
    "    if flattened_histories:\n",
    "        return pd.concat(flattened_histories, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def summarize_changelog_histories(df_histories):\n",
    "    \"\"\"\n",
    "    Summarize flattened changelog histories by counting the number of changes per field.\n",
    "    Returns a DataFrame with one row per issue keyed by 'issue_key'.\n",
    "    \"\"\"\n",
    "    summary = df_histories.groupby('issue_key')['changelog.field'].value_counts().unstack(fill_value=0).reset_index()\n",
    "    summary = summary.rename(columns=lambda x: f'changelog_count_{x}' if x != 'issue_key' else x)\n",
    "    return summary\n",
    "\n",
    "def drop_zero_dominated_columns(df, prefix='changelog_count_', zero_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Drop numeric columns with names starting with the specified prefix if the fraction of zeros exceeds zero_threshold.\n",
    "    \"\"\"\n",
    "    cols_to_drop = []\n",
    "    for col in df.columns:\n",
    "        if col.startswith(prefix) and df[col].dtype.kind in 'biufc':  # numeric types\n",
    "            frac_zeros = (df[col] == 0).mean()\n",
    "            if frac_zeros > zero_threshold:\n",
    "                cols_to_drop.append(col)\n",
    "    if cols_to_drop:\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "    return df\n",
    "\n",
    "def process_issue_links(issuelinks):\n",
    "    \"\"\"\n",
    "    Process the 'fields.issuelinks' JSON array and extract features:\n",
    "      - Total number of links.\n",
    "      - Count and binary flag for each link type.\n",
    "    \"\"\"\n",
    "    features = {\"issuelinks_total\": 0}\n",
    "    link_types = {}\n",
    "    if isinstance(issuelinks, list):\n",
    "        features[\"issuelinks_total\"] = len(issuelinks)\n",
    "        for link in issuelinks:\n",
    "            lt = link.get(\"type\", {}).get(\"name\", \"Unknown\")\n",
    "            link_types[lt] = link_types.get(lt, 0) + 1\n",
    "    else:\n",
    "        features[\"issuelinks_total\"] = 0\n",
    "    for lt, count in link_types.items():\n",
    "        features[f\"issuelinks_{lt.lower()}_count\"] = count\n",
    "        features[f\"has_issuelinks_{lt.lower()}\"] = 1 if count > 0 else 0\n",
    "    return features\n",
    "\n",
    "def process_comments(comments):\n",
    "    \"\"\"\n",
    "    Process the 'fields.comments' JSON array and extract summary features:\n",
    "      - Total number of comments.\n",
    "      - Average and maximum comment length.\n",
    "      - Number of unique authors.\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        \"comment_count\": 0,\n",
    "        \"avg_comment_length\": 0,\n",
    "        \"max_comment_length\": 0,\n",
    "        \"unique_authors_count\": 0\n",
    "    }\n",
    "    if not isinstance(comments, list) or len(comments) == 0:\n",
    "        return features\n",
    "    comment_bodies = [c.get('body', '') for c in comments if isinstance(c, dict)]\n",
    "    authors = [c.get('author', {}).get('name') for c in comments if isinstance(c, dict)]\n",
    "    features[\"comment_count\"] = len(comment_bodies)\n",
    "    lengths = [len(body) for body in comment_bodies]\n",
    "    if lengths:\n",
    "        features[\"avg_comment_length\"] = sum(lengths) / len(lengths)\n",
    "        features[\"max_comment_length\"] = max(lengths)\n",
    "    unique_authors = {a for a in authors if a is not None}\n",
    "    features[\"unique_authors_count\"] = len(unique_authors)\n",
    "    return features\n",
    "\n",
    "def process_repo(jira_name, db, sample_ratio):\n",
    "    \"\"\"\n",
    "    Process a single Jira repository:\n",
    "      - Load issues from MongoDB.\n",
    "      - Sample a fraction of issues.\n",
    "      - Flatten main issue data and apply type conversion.\n",
    "      - Extract and flatten changelog histories, then summarize them (without from/to transitions).\n",
    "      - Merge the changelog summary with the main DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing repository: {jira_name} ...\")\n",
    "    issues = list(db[jira_name].find())\n",
    "    if not issues:\n",
    "        print(f\"⚠️ No documents found for '{jira_name}', skipping.\")\n",
    "        return None\n",
    "    sample_size = max(1, int(len(issues) * sample_ratio))\n",
    "    sampled_issues = random.sample(issues, sample_size)\n",
    "    \n",
    "    # Process main issue structure\n",
    "    df_main = pd.json_normalize(sampled_issues, sep='.')\n",
    "    df_main = fix_data_types(df_main)\n",
    "    \n",
    "    # Process and summarize changelog histories (without from/to transitions)\n",
    "    df_histories = extract_and_flatten_histories(sampled_issues)\n",
    "    if not df_histories.empty:\n",
    "        changelog_summary = summarize_changelog_histories(df_histories)\n",
    "        if \"key\" not in df_main.columns:\n",
    "            df_main[\"key\"] = df_main[\"id\"]\n",
    "        df_merged = pd.merge(df_main, changelog_summary, how=\"left\", left_on=\"key\", right_on=\"issue_key\")\n",
    "        df_merged.drop(columns=[\"issue_key\"], inplace=True, errors='ignore')\n",
    "    else:\n",
    "        df_merged = df_main\n",
    "    return df_merged\n",
    "\n",
    "def drop_high_missing_columns(df, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Drop columns from the DataFrame where the fraction of missing values exceeds the threshold.\n",
    "    \"\"\"\n",
    "    return df.loc[:, df.isnull().mean() <= threshold]\n",
    "\n",
    "def impute_missing_values(df, numeric_strategy='median', categorical_strategy='constant', fill_value='Missing'):\n",
    "    \"\"\"\n",
    "    Impute missing values using scikit-learn's SimpleImputer.\n",
    "      - Numeric columns: impute with the specified strategy (default: median).\n",
    "      - Categorical columns: impute with a constant value (default: \"Missing\").\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        num_imputer = SimpleImputer(strategy=numeric_strategy)\n",
    "        df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
    "    if len(categorical_cols) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy=categorical_strategy, fill_value=fill_value)\n",
    "        df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "    return df\n",
    "\n",
    "def explore_all_fields_in_dtale(selected_jiras=None, sample_ratio=0.2, missing_threshold=0.3, zero_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Connect to the MongoDB 'JiraRepos' database, sample issues from selected repositories,\n",
    "    process and flatten changelog histories (summarizing them without from/to transitions),\n",
    "    process JSON array fields (issuelinks and comments) into engineered features,\n",
    "    drop columns with excessive missing data, impute missing values,\n",
    "    drop changelog summary columns dominated by zeros,\n",
    "    and launch a D-Tale session for interactive visualization.\n",
    "    \"\"\"\n",
    "    client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "    db = client[\"JiraRepos\"]\n",
    "    \n",
    "    with open(\"../0. DataDefinition/jira_data_sources.json\") as f:\n",
    "        jira_data_sources = json.load(f)\n",
    "    \n",
    "    all_jiras = list(jira_data_sources.keys())\n",
    "    if selected_jiras and len(selected_jiras) > 0:\n",
    "        all_jiras = [j for j in all_jiras if j in selected_jiras]\n",
    "        if not all_jiras:\n",
    "            print(f\"⚠️ No valid Jira repositories found for {selected_jiras}.\")\n",
    "            return\n",
    "    \n",
    "    merged_dfs = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_repo, jira_name, db, sample_ratio): jira_name for jira_name in all_jiras}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                merged_dfs.append(result)\n",
    "    \n",
    "    if merged_dfs:\n",
    "        final_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No data to display.\")\n",
    "        return\n",
    "    \n",
    "    # Drop columns with high missing ratios\n",
    "    final_df = drop_high_missing_columns(final_df, threshold=missing_threshold)\n",
    "    \n",
    "    # Process JSON array field for issuelinks\n",
    "    if \"fields.issuelinks\" in final_df.columns:\n",
    "        issuelinks_features = final_df[\"fields.issuelinks\"].apply(process_issue_links)\n",
    "        issuelinks_df = pd.json_normalize(issuelinks_features)\n",
    "        final_df = pd.concat([final_df.drop(columns=[\"fields.issuelinks\"]), issuelinks_df], axis=1)\n",
    "    \n",
    "    # Process JSON array field for comments\n",
    "    if \"fields.comments\" in final_df.columns:\n",
    "        comments_features = final_df[\"fields.comments\"].apply(process_comments)\n",
    "        comments_df = pd.json_normalize(comments_features)\n",
    "        final_df = pd.concat([final_df.drop(columns=[\"fields.comments\"]), comments_df], axis=1)\n",
    "    \n",
    "    # Impute missing values\n",
    "    final_df = impute_missing_values(final_df)\n",
    "    \n",
    "    # Drop changelog summary columns dominated by zeros\n",
    "    final_df = drop_zero_dominated_columns(final_df, prefix='changelog_count_', zero_threshold=zero_threshold)\n",
    "    \n",
    "    print(\"Data processed: types fixed, changelog histories summarized (without from/to transitions), JSON array features engineered, high-missing columns dropped, missing values imputed, and zero-dominated changelog fields dropped.\")\n",
    "    d = dtale.show(final_df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "    d.open_browser()\n",
    "    print(\"✅ D-Tale session launched successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    explore_all_fields_in_dtale([\"Hyperledger\", \"SecondLife\"], sample_ratio=0.2, missing_threshold=0.3, zero_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a349ec42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dtale\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load Sentence Transformer model (you can choose a smaller model if needed)\n",
    "desc_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def fix_data_types(df, numeric_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Convert DataFrame columns (stored as strings) to appropriate data types,\n",
    "    excluding any date formatting.\n",
    "\n",
    "    For each column that does not contain list-like entries:\n",
    "      - If at least `numeric_threshold` fraction of values can be converted to numeric,\n",
    "        the column is converted to a numeric dtype.\n",
    "      - Otherwise, the column is cast to 'category' dtype.\n",
    "    \n",
    "    Note: Date-like strings remain as strings.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].apply(lambda x: isinstance(x, list)).any():\n",
    "            continue\n",
    "        numeric_series = pd.to_numeric(df[col], errors='coerce')\n",
    "        if numeric_series.notnull().mean() >= numeric_threshold:\n",
    "            df[col] = numeric_series\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "def flatten_histories(histories):\n",
    "    \"\"\"\n",
    "    Flatten a list of changelog history entries into a DataFrame.\n",
    "    Each row represents a single change item.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for history in histories:\n",
    "        history_id = history.get(\"id\")\n",
    "        author = history.get(\"author\", {}).get(\"name\")\n",
    "        created = history.get(\"created\")\n",
    "        items = history.get(\"items\", [])\n",
    "        for item in items:\n",
    "            rows.append({\n",
    "                \"changelog.history_id\": history_id,\n",
    "                \"changelog.author\": author,\n",
    "                \"changelog.created\": created,\n",
    "                \"changelog.field\": item.get(\"field\"),\n",
    "                \"changelog.fieldtype\": item.get(\"fieldtype\"),\n",
    "                \"changelog.from\": item.get(\"from\"),\n",
    "                \"changelog.fromString\": item.get(\"fromString\"),\n",
    "                \"changelog.to\": item.get(\"to\"),\n",
    "                \"changelog.toString\": item.get(\"toString\")\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def process_issue_histories(issue):\n",
    "    \"\"\"\n",
    "    Process a single issue's changelog histories by flattening them and applying type conversion.\n",
    "    Adds an 'issue_key' for later merging.\n",
    "    \"\"\"\n",
    "    if \"changelog\" in issue and \"histories\" in issue[\"changelog\"]:\n",
    "        histories = issue[\"changelog\"][\"histories\"]\n",
    "        df_history = flatten_histories(histories)\n",
    "        df_history = fix_data_types(df_history)\n",
    "        df_history[\"issue_key\"] = issue.get(\"key\", issue.get(\"id\"))\n",
    "        return df_history\n",
    "    return None\n",
    "\n",
    "def extract_and_flatten_histories(issues):\n",
    "    \"\"\"\n",
    "    Extract and flatten changelog histories from a list of issues using parallel processing.\n",
    "    \"\"\"\n",
    "    flattened_histories = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_issue_histories, issue): issue for issue in issues}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None and not result.empty:\n",
    "                flattened_histories.append(result)\n",
    "    if flattened_histories:\n",
    "        return pd.concat(flattened_histories, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def summarize_changelog_histories(df_histories):\n",
    "    \"\"\"\n",
    "    Summarize flattened changelog histories by counting the number of changes per field.\n",
    "    Returns a DataFrame with one row per issue keyed by 'issue_key'.\n",
    "    \"\"\"\n",
    "    summary = df_histories.groupby('issue_key')['changelog.field'].value_counts().unstack(fill_value=0).reset_index()\n",
    "    summary = summary.rename(columns=lambda x: f'changelog_count_{x}' if x != 'issue_key' else x)\n",
    "    return summary\n",
    "\n",
    "def drop_zero_dominated_columns(df, prefix='changelog_count_', zero_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Drop numeric columns with names starting with the specified prefix if the fraction of zeros exceeds zero_threshold.\n",
    "    \"\"\"\n",
    "    cols_to_drop = []\n",
    "    for col in df.columns:\n",
    "        if col.startswith(prefix) and df[col].dtype.kind in 'biufc':  # numeric types\n",
    "            frac_zeros = (df[col] == 0).mean()\n",
    "            if frac_zeros > zero_threshold:\n",
    "                cols_to_drop.append(col)\n",
    "    if cols_to_drop:\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "    return df\n",
    "\n",
    "def process_issue_links(issuelinks):\n",
    "    \"\"\"\n",
    "    Process the 'fields.issuelinks' JSON array and extract features:\n",
    "      - Total number of links.\n",
    "      - Count and binary flag for each link type.\n",
    "    \"\"\"\n",
    "    features = {\"issuelinks_total\": 0}\n",
    "    link_types = {}\n",
    "    if isinstance(issuelinks, list):\n",
    "        features[\"issuelinks_total\"] = len(issuelinks)\n",
    "        for link in issuelinks:\n",
    "            lt = link.get(\"type\", {}).get(\"name\", \"Unknown\")\n",
    "            link_types[lt] = link_types.get(lt, 0) + 1\n",
    "    else:\n",
    "        features[\"issuelinks_total\"] = 0\n",
    "    for lt, count in link_types.items():\n",
    "        features[f\"issuelinks_{lt.lower()}_count\"] = count\n",
    "        features[f\"has_issuelinks_{lt.lower()}\"] = 1 if count > 0 else 0\n",
    "    return features\n",
    "\n",
    "def process_comments(comments):\n",
    "    \"\"\"\n",
    "    Process the 'fields.comments' JSON array and extract summary features:\n",
    "      - Total number of comments.\n",
    "      - Average and maximum comment length.\n",
    "      - Number of unique authors.\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        \"comment_count\": 0,\n",
    "        \"avg_comment_length\": 0,\n",
    "        \"max_comment_length\": 0,\n",
    "        \"unique_authors_count\": 0\n",
    "    }\n",
    "    if not isinstance(comments, list) or len(comments) == 0:\n",
    "        return features\n",
    "    comment_bodies = [c.get('body', '') for c in comments if isinstance(c, dict)]\n",
    "    authors = [c.get('author', {}).get('name') for c in comments if isinstance(c, dict)]\n",
    "    features[\"comment_count\"] = len(comment_bodies)\n",
    "    lengths = [len(body) for body in comment_bodies]\n",
    "    if lengths:\n",
    "        features[\"avg_comment_length\"] = sum(lengths) / len(lengths)\n",
    "        features[\"max_comment_length\"] = max(lengths)\n",
    "    unique_authors = {a for a in authors if a is not None}\n",
    "    features[\"unique_authors_count\"] = len(unique_authors)\n",
    "    return features\n",
    "\n",
    "def process_description_field(descriptions):\n",
    "    \"\"\"\n",
    "    Process the 'fields.description' field by generating dense embeddings\n",
    "    using a pre-trained Sentence Transformer. The resulting embedding vector\n",
    "    is expanded into multiple columns (one per dimension).\n",
    "    \"\"\"\n",
    "    # Ensure descriptions are strings and fill missing values with empty string\n",
    "    descriptions = descriptions.fillna(\"\").astype(str)\n",
    "    embeddings = descriptions.apply(lambda x: desc_model.encode(x, show_progress_bar=False))\n",
    "    emb_array = np.vstack(embeddings.values)\n",
    "    emb_df = pd.DataFrame(emb_array, index=descriptions.index, \n",
    "                          columns=[f\"desc_emb_{i}\" for i in range(emb_array.shape[1])])\n",
    "    return emb_df\n",
    "\n",
    "def process_repo(jira_name, db, sample_ratio):\n",
    "    \"\"\"\n",
    "    Process a single Jira repository:\n",
    "      - Load issues from MongoDB.\n",
    "      - Sample a fraction of issues.\n",
    "      - Flatten main issue data and apply type conversion.\n",
    "      - Extract and flatten changelog histories, then summarize them (without from/to transitions).\n",
    "      - Merge the changelog summary with the main DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing repository: {jira_name} ...\")\n",
    "    issues = list(db[jira_name].find())\n",
    "    if not issues:\n",
    "        print(f\"⚠️ No documents found for '{jira_name}', skipping.\")\n",
    "        return None\n",
    "    sample_size = max(1, int(len(issues) * sample_ratio))\n",
    "    sampled_issues = random.sample(issues, sample_size)\n",
    "    \n",
    "    # Process main issue structure\n",
    "    df_main = pd.json_normalize(sampled_issues, sep='.')\n",
    "    df_main = fix_data_types(df_main)\n",
    "    \n",
    "    # Process and summarize changelog histories (without from/to transitions)\n",
    "    df_histories = extract_and_flatten_histories(sampled_issues)\n",
    "    if not df_histories.empty:\n",
    "        changelog_summary = summarize_changelog_histories(df_histories)\n",
    "        if \"key\" not in df_main.columns:\n",
    "            df_main[\"key\"] = df_main[\"id\"]\n",
    "        df_main = pd.merge(df_main, changelog_summary, how=\"left\", left_on=\"key\", right_on=\"issue_key\")\n",
    "        df_main.drop(columns=[\"issue_key\"], inplace=True, errors='ignore')\n",
    "    \n",
    "    return df_main\n",
    "\n",
    "def drop_high_missing_columns(df, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Drop columns from the DataFrame where the fraction of missing values exceeds the threshold.\n",
    "    \"\"\"\n",
    "    return df.loc[:, df.isnull().mean() <= threshold]\n",
    "\n",
    "def impute_missing_values(df, numeric_strategy='median', categorical_strategy='constant', fill_value='Missing'):\n",
    "    \"\"\"\n",
    "    Impute missing values using scikit-learn's SimpleImputer.\n",
    "      - Numeric columns: impute with the specified strategy (default: median).\n",
    "      - Categorical columns: impute with a constant value (default: \"Missing\").\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        num_imputer = SimpleImputer(strategy=numeric_strategy)\n",
    "        df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
    "    if len(categorical_cols) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy=categorical_strategy, fill_value=fill_value)\n",
    "        df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "    return df\n",
    "\n",
    "def explore_all_fields_in_dtale(selected_jiras=None, sample_ratio=0.2, missing_threshold=0.3, zero_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Connect to the MongoDB 'JiraRepos' database, sample issues from selected repositories,\n",
    "    process and flatten changelog histories (summarizing them without from/to transitions),\n",
    "    process JSON array fields (issuelinks, comments) into engineered features,\n",
    "    process the 'fields.description' field into dense embedding features,\n",
    "    drop columns with excessive missing data, impute missing values,\n",
    "    drop changelog summary columns dominated by zeros,\n",
    "    and launch a D-Tale session for interactive visualization.\n",
    "    \"\"\"\n",
    "    client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "    db = client[\"JiraRepos\"]\n",
    "    \n",
    "    with open(\"../0. DataDefinition/jira_data_sources.json\") as f:\n",
    "        jira_data_sources = json.load(f)\n",
    "    \n",
    "    all_jiras = list(jira_data_sources.keys())\n",
    "    if selected_jiras and len(selected_jiras) > 0:\n",
    "        all_jiras = [j for j in all_jiras if j in selected_jiras]\n",
    "        if not all_jiras:\n",
    "            print(f\"⚠️ No valid Jira repositories found for {selected_jiras}.\")\n",
    "            return\n",
    "    \n",
    "    merged_dfs = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_repo, jira_name, db, sample_ratio): jira_name for jira_name in all_jiras}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                merged_dfs.append(result)\n",
    "    \n",
    "    if merged_dfs:\n",
    "        final_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No data to display.\")\n",
    "        return\n",
    "    \n",
    "    # Drop columns with high missing ratios\n",
    "    final_df = drop_high_missing_columns(final_df, threshold=missing_threshold)\n",
    "    \n",
    "    # Process JSON array field for issuelinks\n",
    "    if \"fields.issuelinks\" in final_df.columns:\n",
    "        issuelinks_features = final_df[\"fields.issuelinks\"].apply(process_issue_links)\n",
    "        issuelinks_df = pd.json_normalize(issuelinks_features)\n",
    "        final_df = pd.concat([final_df.drop(columns=[\"fields.issuelinks\"]), issuelinks_df], axis=1)\n",
    "    \n",
    "    # Process JSON array field for comments\n",
    "    if \"fields.comments\" in final_df.columns:\n",
    "        comments_features = final_df[\"fields.comments\"].apply(process_comments)\n",
    "        comments_df = pd.json_normalize(comments_features)\n",
    "        final_df = pd.concat([final_df.drop(columns=[\"fields.comments\"]), comments_df], axis=1)\n",
    "    \n",
    "    # Process the 'fields.description' field to create dense embeddings\n",
    "    if \"fields.description\" in final_df.columns:\n",
    "        desc_embeddings = process_description_field(final_df[\"fields.description\"])\n",
    "        final_df = pd.concat([final_df.drop(columns=[\"fields.description\"]), desc_embeddings], axis=1)\n",
    "    \n",
    "    # Impute missing values\n",
    "    final_df = impute_missing_values(final_df)\n",
    "    \n",
    "    # Drop changelog summary columns dominated by zeros\n",
    "    final_df = drop_zero_dominated_columns(final_df, prefix='changelog_count_', zero_threshold=zero_threshold)\n",
    "    \n",
    "    print(\"Data processed: types fixed, changelog histories summarized, JSON array features engineered, description embeddings added, high-missing columns dropped, missing values imputed, and zero-dominated changelog fields dropped.\")\n",
    "    d = dtale.show(final_df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "    d.open_browser()\n",
    "    print(\"✅ D-Tale session launched successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    explore_all_fields_in_dtale([\"Hyperledger\", \"SecondLife\"], sample_ratio=0.2, missing_threshold=0.3, zero_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed4378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing repository: Hyperledger ...\n",
      "\n",
      "Processing repository: SecondLife ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_51147/1401601040.py:276: FutureWarning:\n",
      "\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n",
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_51147/1401601040.py:276: FutureWarning:\n",
      "\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n",
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_51147/1401601040.py:276: FutureWarning:\n",
      "\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed. Launching D-Tale session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ D-Tale session launched successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import dtale\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import dateparser\n",
    "from pymongo import MongoClient\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the Sentence Transformer model (you can choose a smaller one if needed)\n",
    "desc_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def convert_date_columns_dateparser(df, date_columns):\n",
    "    \"\"\"\n",
    "    Convert the specified date columns from string to datetime using dateparser.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input DataFrame containing date strings.\n",
    "      date_columns (list): List of column names to convert.\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: The DataFrame with specified columns converted to datetime objects.\n",
    "    \"\"\"\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(lambda x: dateparser.parse(x, \n",
    "                                    settings={'RETURN_AS_TIMEZONE_AWARE': True, 'TIMEZONE': 'UTC'})\n",
    "                                    if pd.notnull(x) else pd.NaT)\n",
    "    return df\n",
    "\n",
    "\n",
    "def fix_data_types(df, numeric_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Convert DataFrame columns (stored as strings) to appropriate data types,\n",
    "    excluding any date formatting.\n",
    "\n",
    "    For each column that is not list-like:\n",
    "      - If at least `numeric_threshold` fraction of values can be converted to numeric,\n",
    "        the column is converted to a numeric dtype.\n",
    "      - Otherwise, the column is cast to 'category' dtype.\n",
    "    (Date-like strings remain as strings.)\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].apply(lambda x: isinstance(x, list)).any():\n",
    "            continue\n",
    "        numeric_series = pd.to_numeric(df[col], errors='coerce')\n",
    "        if numeric_series.notnull().mean() >= numeric_threshold:\n",
    "            df[col] = numeric_series\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "\n",
    "def flatten_histories(histories):\n",
    "    \"\"\"\n",
    "    Flatten a list of changelog history entries into a DataFrame.\n",
    "    Each row represents a single change item.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for history in histories:\n",
    "        history_id = history.get(\"id\")\n",
    "        author = history.get(\"author\", {}).get(\"name\")\n",
    "        created = history.get(\"created\")\n",
    "        items = history.get(\"items\", [])\n",
    "        for item in items:\n",
    "            rows.append({\n",
    "                \"changelog.history_id\": history_id,\n",
    "                \"changelog.author\": author,\n",
    "                \"changelog.created\": created,\n",
    "                \"changelog.field\": item.get(\"field\"),\n",
    "                \"changelog.fieldtype\": item.get(\"fieldtype\"),\n",
    "                \"changelog.from\": item.get(\"from\"),\n",
    "                \"changelog.fromString\": item.get(\"fromString\"),\n",
    "                \"changelog.to\": item.get(\"to\"),\n",
    "                \"changelog.toString\": item.get(\"toString\")\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def process_issue_histories(issue):\n",
    "    \"\"\"\n",
    "    Process a single issue's changelog histories:\n",
    "      - Flatten the histories.\n",
    "      - Apply type conversion.\n",
    "      - Add an 'issue_key' (using 'key' if available, else 'id') for merging.\n",
    "    \"\"\"\n",
    "    if \"changelog\" in issue and \"histories\" in issue[\"changelog\"]:\n",
    "        histories = issue[\"changelog\"][\"histories\"]\n",
    "        df_history = flatten_histories(histories)\n",
    "        df_history = fix_data_types(df_history)\n",
    "        df_history[\"issue_key\"] = issue.get(\"key\", issue.get(\"id\"))\n",
    "        return df_history\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_and_flatten_histories(issues):\n",
    "    \"\"\"\n",
    "    Extract and flatten changelog histories from a list of issues using parallel processing.\n",
    "    \"\"\"\n",
    "    flattened_histories = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_issue_histories, issue): issue for issue in issues}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None and not result.empty:\n",
    "                flattened_histories.append(result)\n",
    "    if flattened_histories:\n",
    "        return pd.concat(flattened_histories, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def summarize_changelog_histories(df_histories):\n",
    "    \"\"\"\n",
    "    Summarize flattened changelog histories by counting the number of changes per field.\n",
    "    Returns a DataFrame with one row per issue (keyed by 'issue_key').\n",
    "    \"\"\"\n",
    "    summary = df_histories.groupby('issue_key')['changelog.field'].value_counts().unstack(fill_value=0).reset_index()\n",
    "    summary = summary.rename(columns=lambda x: f'changelog_count_{x}' if x != 'issue_key' else x)\n",
    "    return summary\n",
    "\n",
    "\n",
    "def drop_zero_dominated_columns(df, prefix='changelog_count_', zero_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Drop numeric columns with names starting with `prefix` if more than `zero_threshold`\n",
    "    fraction of their values are zeros.\n",
    "    \"\"\"\n",
    "    cols_to_drop = []\n",
    "    for col in df.columns:\n",
    "        if col.startswith(prefix) and df[col].dtype.kind in 'biufc':\n",
    "            frac_zeros = (df[col] == 0).mean()\n",
    "            if frac_zeros > zero_threshold:\n",
    "                cols_to_drop.append(col)\n",
    "    if cols_to_drop:\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_issue_links(issuelinks):\n",
    "    \"\"\"\n",
    "    Process the 'fields.issuelinks' JSON array and extract features:\n",
    "      - Total number of links.\n",
    "      - Count and binary flag for each link type.\n",
    "    \"\"\"\n",
    "    features = {\"issuelinks_total\": 0}\n",
    "    link_types = {}\n",
    "    if isinstance(issuelinks, list):\n",
    "        features[\"issuelinks_total\"] = len(issuelinks)\n",
    "        for link in issuelinks:\n",
    "            lt = link.get(\"type\", {}).get(\"name\", \"Unknown\")\n",
    "            link_types[lt] = link_types.get(lt, 0) + 1\n",
    "    else:\n",
    "        features[\"issuelinks_total\"] = 0\n",
    "    for lt, count in link_types.items():\n",
    "        features[f\"issuelinks_{lt.lower()}_count\"] = count\n",
    "        features[f\"has_issuelinks_{lt.lower()}\"] = 1 if count > 0 else 0\n",
    "    return features\n",
    "\n",
    "\n",
    "def process_comments(comments):\n",
    "    \"\"\"\n",
    "    Process the 'fields.comments' JSON array and extract summary features:\n",
    "      - Total number of comments.\n",
    "      - Average and maximum comment length.\n",
    "      - Number of unique authors.\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        \"comment_count\": 0,\n",
    "        \"avg_comment_length\": 0,\n",
    "        \"max_comment_length\": 0,\n",
    "        \"unique_authors_count\": 0\n",
    "    }\n",
    "    if not isinstance(comments, list) or len(comments) == 0:\n",
    "        return features\n",
    "    comment_bodies = [c.get('body', '') for c in comments if isinstance(c, dict)]\n",
    "    authors = [c.get('author', {}).get('name') for c in comments if isinstance(c, dict)]\n",
    "    features[\"comment_count\"] = len(comment_bodies)\n",
    "    lengths = [len(body) for body in comment_bodies]\n",
    "    if lengths:\n",
    "        features[\"avg_comment_length\"] = sum(lengths) / len(lengths)\n",
    "        features[\"max_comment_length\"] = max(lengths)\n",
    "    unique_authors = {a for a in authors if a is not None}\n",
    "    features[\"unique_authors_count\"] = len(unique_authors)\n",
    "    return features\n",
    "\n",
    "\n",
    "def process_description_field(descriptions):\n",
    "    \"\"\"\n",
    "    Process the 'fields.description' field by generating dense embeddings\n",
    "    using a pre-trained Sentence Transformer. The resulting embedding vector\n",
    "    is expanded into multiple columns (one per dimension).\n",
    "    \"\"\"\n",
    "    descriptions = descriptions.fillna(\"\").astype(str)\n",
    "    embeddings = descriptions.apply(lambda x: desc_model.encode(x, show_progress_bar=False))\n",
    "    emb_array = np.vstack(embeddings.values)\n",
    "    emb_df = pd.DataFrame(emb_array, index=descriptions.index,\n",
    "                          columns=[f\"desc_emb_{i}\" for i in range(emb_array.shape[1])])\n",
    "    return emb_df\n",
    "\n",
    "\n",
    "def process_repo(jira_name, db, sample_ratio):\n",
    "    \"\"\"\n",
    "    Process a single Jira repository:\n",
    "      - Load issues from MongoDB.\n",
    "      - Sample a fraction of issues.\n",
    "      - Flatten main issue data and apply type conversion.\n",
    "      - Extract and flatten changelog histories, then summarize them (without from/to transitions).\n",
    "      - Merge the changelog summary with the main DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing repository: {jira_name} ...\")\n",
    "    issues = list(db[jira_name].find())\n",
    "    if not issues:\n",
    "        print(f\"⚠️ No documents found for '{jira_name}', skipping.\")\n",
    "        return None\n",
    "    sample_size = max(1, int(len(issues) * sample_ratio))\n",
    "    sampled_issues = random.sample(issues, sample_size)\n",
    "    \n",
    "    df_main = pd.json_normalize(sampled_issues, sep='.')\n",
    "    df_main = fix_data_types(df_main)\n",
    "    \n",
    "    df_histories = extract_and_flatten_histories(sampled_issues)\n",
    "    if not df_histories.empty:\n",
    "        changelog_summary = summarize_changelog_histories(df_histories)\n",
    "        if \"key\" not in df_main.columns:\n",
    "            df_main[\"key\"] = df_main[\"id\"]\n",
    "        df_main = pd.merge(df_main, changelog_summary, how=\"left\", left_on=\"key\", right_on=\"issue_key\")\n",
    "        df_main.drop(columns=[\"issue_key\"], inplace=True, errors='ignore')\n",
    "    \n",
    "    return df_main\n",
    "\n",
    "\n",
    "def drop_high_missing_columns(df, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Drop columns from the DataFrame where the fraction of missing values exceeds the threshold.\n",
    "    \"\"\"\n",
    "    return df.loc[:, df.isnull().mean() <= threshold]\n",
    "\n",
    "\n",
    "def impute_missing_values(df, numeric_strategy='median', categorical_strategy='constant', fill_value='Missing'):\n",
    "    \"\"\"\n",
    "    Impute missing values using scikit-learn's SimpleImputer.\n",
    "      - Numeric columns: impute with the specified strategy (default: median).\n",
    "      - Categorical columns: impute with a constant value (default: \"Missing\").\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        num_imputer = SimpleImputer(strategy=numeric_strategy)\n",
    "        df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
    "    if len(categorical_cols) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy=categorical_strategy, fill_value=fill_value)\n",
    "        df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "    return df\n",
    "\n",
    "\n",
    "def explore_all_fields_in_dtale(selected_jiras=None, sample_ratio=0.2, missing_threshold=0.3,\n",
    "                                zero_threshold=0.8, open_dtale=True):\n",
    "    \"\"\"\n",
    "    Connect to the MongoDB 'JiraRepos' database, sample issues from selected repositories,\n",
    "    process and flatten changelog histories (summarizing them without from/to transitions),\n",
    "    process JSON array fields (issuelinks, comments) into engineered features,\n",
    "    process the 'fields.description' field into dense embedding features,\n",
    "    drop columns with excessive missing data, impute missing values,\n",
    "    and drop changelog summary columns dominated by zeros.\n",
    "    \n",
    "    If open_dtale is True, launch a D-Tale session for interactive visualization;\n",
    "    otherwise, simply return the final DataFrame.\n",
    "    \"\"\"\n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "    db = client[\"JiraRepos\"]\n",
    "\n",
    "    # Load Jira data sources configuration\n",
    "    with open(\"../0. DataDefinition/jira_data_sources.json\") as f:\n",
    "        jira_data_sources = json.load(f)\n",
    "\n",
    "    all_jiras = list(jira_data_sources.keys())\n",
    "    if selected_jiras and len(selected_jiras) > 0:\n",
    "        all_jiras = [j for j in all_jiras if j in selected_jiras]\n",
    "        if not all_jiras:\n",
    "            print(f\"⚠️ No valid Jira repositories found for {selected_jiras}.\")\n",
    "            return\n",
    "\n",
    "    # Process each repository in parallel\n",
    "    merged_dfs = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_repo, jira_name, db, sample_ratio): jira_name for jira_name in all_jiras}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                merged_dfs.append(result)\n",
    "\n",
    "    if merged_dfs:\n",
    "        final_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No data to display.\")\n",
    "        return\n",
    "\n",
    "    # Drop columns with high missing ratios\n",
    "    final_df = drop_high_missing_columns(final_df, threshold=missing_threshold)\n",
    "\n",
    "    # Process JSON array field for issuelinks\n",
    "    if \"fields.issuelinks\" in final_df.columns:\n",
    "        issuelinks_features = final_df[\"fields.issuelinks\"].apply(process_issue_links)\n",
    "        issuelinks_df = pd.json_normalize(issuelinks_features)\n",
    "        final_df = pd.concat([final_df.drop(columns=[\"fields.issuelinks\"]), issuelinks_df], axis=1)\n",
    "\n",
    "    # Process JSON array field for comments\n",
    "    if \"fields.comments\" in final_df.columns:\n",
    "        comments_features = final_df[\"fields.comments\"].apply(process_comments)\n",
    "        comments_df = pd.json_normalize(comments_features)\n",
    "        final_df = pd.concat([final_df.drop(columns=[\"fields.comments\"]), comments_df], axis=1)\n",
    "\n",
    "    # Process the 'fields.description' field to generate dense embeddings\n",
    "    if \"fields.description\" in final_df.columns:\n",
    "        desc_embeddings = process_description_field(final_df[\"fields.description\"])\n",
    "        final_df = pd.concat([final_df.drop(columns=[\"fields.description\"]), desc_embeddings], axis=1)\n",
    "\n",
    "    # Impute missing values\n",
    "    final_df = impute_missing_values(final_df)\n",
    "\n",
    "    # Drop changelog summary columns dominated by zeros\n",
    "    final_df = drop_zero_dominated_columns(final_df, prefix='changelog_count_', zero_threshold=zero_threshold)\n",
    "\n",
    "    if open_dtale:\n",
    "        print(\"Data processed. Launching D-Tale session...\")\n",
    "        d = dtale.show(final_df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "        d.open_browser()\n",
    "        print(\"✅ D-Tale session launched successfully.\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def export_final_df():\n",
    "    \"\"\"\n",
    "    Run the full OverviewAnalysis pipeline and return the final DataFrame with all engineered features.\n",
    "    This version does not launch D-Tale (open_dtale=False) so that it can be used directly as the training dataset.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The final processed DataFrame ready for training.\n",
    "    \"\"\"\n",
    "    final_df = explore_all_fields_in_dtale(\n",
    "        selected_jiras=[\"Hyperledger\", \"SecondLife\"],\n",
    "        sample_ratio=0.05,\n",
    "        missing_threshold=0.3,\n",
    "        zero_threshold=0.8,\n",
    "        open_dtale=True\n",
    "    )\n",
    "    return final_df\n",
    "\n",
    "\n",
    "# For testing purposes, you can run export_final_df() if executing this module directly.\n",
    "if __name__ == \"__main__\":\n",
    "    df_for_training = export_final_df()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
