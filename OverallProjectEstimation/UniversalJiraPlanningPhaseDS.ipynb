{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "project_root = os.path.abspath(\"..\")  # adjust based on your directory structure\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    \n",
    "import re\n",
    "import pandas as pd\n",
    "from FeatureCleaning.CleanDSDTale import export_clean_df  # Adjust import path as needed\n",
    "import dtale\n",
    "\n",
    "def extract_sprint_name(sprint_str):\n",
    "    \"\"\"\n",
    "    Extract the sprint name from a sprint string.\n",
    "    For example, from a value like:\n",
    "      \"com.atlassian.greenhopper.service.sprint.Sprint@16353814[id=5599,...,name=Sprint 9,...]\"\n",
    "    this returns \"Sprint 9\".\n",
    "    \"\"\"\n",
    "    if not sprint_str or not isinstance(sprint_str, str):\n",
    "        return None\n",
    "    match = re.search(r'name=([^,]+)', sprint_str)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def extract_sprint_from_fixversions(fix_versions):\n",
    "    \"\"\"\n",
    "    Given a list of fixVersions dictionaries, return the first version name containing 'Sprint'.\n",
    "    \"\"\"\n",
    "    if isinstance(fix_versions, list):\n",
    "        for version in fix_versions:\n",
    "            name = version.get(\"name\", \"\")\n",
    "            if \"Sprint\" in name:\n",
    "                return name\n",
    "    return None\n",
    "\n",
    "def add_sprint_field(df):\n",
    "    \"\"\"\n",
    "    Add a standardized 'sprint' column to the DataFrame.\n",
    "    First, try using customfield_10557; if not present, fall back to fixVersions.\n",
    "    \"\"\"\n",
    "    if \"fields.customfield_10557\" in df.columns:\n",
    "        df[\"sprint\"] = df[\"fields.customfield_10557\"].apply(extract_sprint_name)\n",
    "    elif \"fields.fixVersions\" in df.columns:\n",
    "        df[\"sprint\"] = df[\"fields.fixVersions\"].apply(extract_sprint_from_fixversions)\n",
    "    else:\n",
    "        df[\"sprint\"] = None\n",
    "    return df\n",
    "\n",
    "def extract_planning_fields(df):\n",
    "    \"\"\"\n",
    "    From the full cleaned DataFrame, extract only the planning-phase fields.\n",
    "    Enhanced to include all critical planning-phase data for task-level and project-level estimation.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = add_sprint_field(df)\n",
    "    \n",
    "    planning_cols = [\n",
    "        # Basic issue identification\n",
    "        \"key\",                    # Issue key\n",
    "        \"fields.summary\",         # Summary text\n",
    "        \n",
    "        # Issue classification\n",
    "        \"fields.issuetype.name\",  # Type\n",
    "        \"fields.status.name\",     # Status\n",
    "        \"fields.priority.name\",   # Priority\n",
    "        \n",
    "        # People involved\n",
    "        \"fields.assignee.key\",    # Assignee ID\n",
    "        \"fields.creator.key\",     # Creator ID\n",
    "        \"fields.reporter.key\",    # Reporter ID\n",
    "        \n",
    "        # Project context\n",
    "        \"fields.project.id\",      # Project ID\n",
    "        \"fields.project.name\",    # Project name\n",
    "        \n",
    "        # Time information\n",
    "        \"fields.created\",         # Creation date\n",
    "        \n",
    "        # Relationships\n",
    "        \"sprint\",                 # Extracted sprint info\n",
    "        \"issuelinks_total\",       # Total link count\n",
    "        \"has_issuelinks_relates\", # Has 'relates to' links\n",
    "        \"has_issuelinks_cloners\", # Has 'cloners' links\n",
    "        \n",
    "        # Components and labels\n",
    "        \"fields.components\",      # Components\n",
    "        \"fields.labels\",          # Labels\n",
    "        \n",
    "        # For training purposes\n",
    "        \"fields.resolutiondate\"   # Used to calculate resolution time\n",
    "    ]\n",
    "    \n",
    "    # Get all description embedding columns (they start with \"desc_emb_\")\n",
    "    embedding_cols = [col for col in df.columns if col.startswith(\"desc_emb_\")]\n",
    "    \n",
    "    # Combine all desired columns\n",
    "    all_cols = planning_cols + embedding_cols\n",
    "    \n",
    "    # Filter to only include columns that exist in the DataFrame\n",
    "    available_cols = [col for col in all_cols if col in df.columns]\n",
    "    \n",
    "    # Create the base planning DataFrame\n",
    "    planning_df = df[available_cols].copy()\n",
    "    \n",
    "    # Add derived features\n",
    "    \n",
    "    # Component and label counts\n",
    "    if \"fields.components\" in planning_df.columns:\n",
    "        planning_df[\"component_count\"] = planning_df[\"fields.components\"].apply(\n",
    "            lambda x: len(x) if isinstance(x, list) else 0\n",
    "        )\n",
    "    \n",
    "    if \"fields.labels\" in planning_df.columns:\n",
    "        planning_df[\"label_count\"] = planning_df[\"fields.labels\"].apply(\n",
    "            lambda x: len(x) if isinstance(x, list) else 0\n",
    "        )\n",
    "    \n",
    "    # Creation date features\n",
    "    if \"fields.created\" in planning_df.columns:\n",
    "        planning_df[\"created_date\"] = pd.to_datetime(planning_df[\"fields.created\"])\n",
    "        planning_df[\"created_day_of_week\"] = planning_df[\"created_date\"].dt.dayofweek\n",
    "        planning_df[\"created_month\"] = planning_df[\"created_date\"].dt.month\n",
    "        planning_df[\"created_year\"] = planning_df[\"created_date\"].dt.year\n",
    "    \n",
    "    # Calculate resolution time (if available - for training only)\n",
    "    if \"fields.resolutiondate\" in planning_df.columns and \"fields.created\" in planning_df.columns:\n",
    "        planning_df[\"resolution_time\"] = (\n",
    "            pd.to_datetime(planning_df[\"fields.resolutiondate\"]) - \n",
    "            pd.to_datetime(planning_df[\"fields.created\"])\n",
    "        ).dt.total_seconds() / 3600  # Convert to hours\n",
    "        \n",
    "        # Drop the raw resolution date as we now have the derived feature\n",
    "        planning_df = planning_df.drop(columns=[\"fields.resolutiondate\"])\n",
    "    \n",
    "    return planning_df\n",
    "\n",
    "def export_clean_planningphase_df(open_dtale=True):\n",
    "    \"\"\"\n",
    "    Run the full cleaning pipeline to obtain a cleaned task-level DataFrame,\n",
    "    then extract only the planning-phase fields to create a planning-phase dataset.\n",
    "    If open_dtale is True, launch a D-Tale session for interactive exploration.\n",
    "    \n",
    "    Returns:\n",
    "      planning_df (pd.DataFrame): The planning-phase DataFrame with the selected fields.\n",
    "    \"\"\"\n",
    "    full_df = export_clean_df()  # This returns your fully cleaned task-level DataFrame.\n",
    "    planning_df = extract_planning_fields(full_df)\n",
    "\n",
    "    \n",
    "    if open_dtale:\n",
    "        print(\"Launching D-Tale session for planning-phase DataFrame...\")\n",
    "        d_pf = dtale.show(planning_df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "\n",
    "        d_pf.open_browser()\n",
    "        \n",
    "    return planning_df\n",
    "\n",
    "def expand_categorical_features(df):\n",
    "    \"\"\"\n",
    "    Expand categorical distributions stored as dictionaries.\n",
    "    \"\"\"\n",
    "    # Identify columns that contain dictionaries\n",
    "    dict_cols = [col for col in df.columns \n",
    "                if isinstance(df[col].iloc[0], dict) if len(df) > 0]\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    \n",
    "    for col in dict_cols:\n",
    "        # Find all unique categories across all projects\n",
    "        all_categories = set()\n",
    "        for d in df[col]:\n",
    "            if isinstance(d, dict):\n",
    "                all_categories.update(d.keys())\n",
    "        \n",
    "        # Create a column for each category\n",
    "        for category in all_categories:\n",
    "            new_col_name = f\"{col}_{category}\"\n",
    "            result_df[new_col_name] = df[col].apply(\n",
    "                lambda x: x.get(category, 0) if isinstance(x, dict) else 0\n",
    "            )\n",
    "        \n",
    "        # Drop the original dictionary column\n",
    "        result_df = result_df.drop(columns=[col])\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def create_project_features(df):\n",
    "    \"\"\"\n",
    "    Aggregate task-level features to create project-level features.\n",
    "    \"\"\"\n",
    "    # Group by project ID and name\n",
    "    project_features = df.groupby(['fields.project.id', 'fields.project.name']).agg({\n",
    "        # Issue counts\n",
    "        'key': 'count',  # Total number of issues\n",
    "        \n",
    "        # Issue type distribution\n",
    "        'fields.issuetype.name': lambda x: x.value_counts().to_dict(),\n",
    "        \n",
    "        # Priority distribution\n",
    "        'fields.priority.name': lambda x: x.value_counts().to_dict(),\n",
    "        \n",
    "        # Status distribution\n",
    "        'fields.status.name': lambda x: x.value_counts().to_dict(),\n",
    "        \n",
    "        # Team size metrics\n",
    "        'fields.assignee.key': lambda x: x.nunique(),  # Number of unique assignees\n",
    "        'fields.creator.key': lambda x: x.nunique(),   # Number of unique creators\n",
    "        \n",
    "        # Connectivity metrics\n",
    "        'issuelinks_total': ['sum', 'mean'],  # Link density\n",
    "        'has_issuelinks_relates': 'mean',     # Percentage with related links\n",
    "        \n",
    "        # Component usage\n",
    "        'fields.components': lambda x: set(item for sublist in x if isinstance(sublist, list) for item in sublist),\n",
    "        \n",
    "        # Description embedding averages\n",
    "        **{f'desc_emb_{i}': 'mean' for i in range(82)}  # Average embeddings across all tasks\n",
    "    })\n",
    "    \n",
    "    # Flatten the multi-level columns\n",
    "    project_features.columns = ['_'.join(col) if isinstance(col, tuple) else col \n",
    "                               for col in project_features.columns]\n",
    "    \n",
    "    # Expand the categorical distributions\n",
    "    project_features = expand_categorical_features(project_features)\n",
    "    \n",
    "    # Add component count\n",
    "    project_features['component_count'] = project_features['fields.components'].apply(len)\n",
    "    \n",
    "    # Compute complexity indicators\n",
    "    project_features['issue_to_assignee_ratio'] = (\n",
    "        project_features['key_count'] / project_features['fields.assignee.key_nunique']\n",
    "    )\n",
    "    \n",
    "    # Calculate link density (connectedness of the task graph)\n",
    "    project_features['link_density'] = (\n",
    "        project_features['issuelinks_total_sum'] / project_features['key_count']\n",
    "    )\n",
    "    \n",
    "    return project_features\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    df_planning = export_clean_planningphase_df(open_dtale=True)\n",
    "    print(\"Planning-phase DataFrame columns:\")\n",
    "    print(df_planning.columns.tolist())\n",
    "    print(\"Sample rows:\")\n",
    "    print(df_planning.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing repository: Hyperledger ...\n",
      "\n",
      "Processing repository: SecondLife ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegodias/Documents/Projects/JiraDataset/FeatureCleaning/CleanDSDTale.py:394: FutureWarning:\n",
      "\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n",
      "/Users/diegodias/Documents/Projects/JiraDataset/FeatureCleaning/CleanDSDTale.py:394: FutureWarning:\n",
      "\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n",
      "/Users/diegodias/Documents/Projects/JiraDataset/FeatureCleaning/CleanDSDTale.py:394: FutureWarning:\n",
      "\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed. Launching D-Tale session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2011-04-11 10:43:13', '2012-12-17 15:41:37', '2012-01-25 15:00:30',\n",
      " '2011-01-16 17:32:25', '2011-01-14 09:54:49', '2011-08-04 19:01:01',\n",
      " '2010-11-01 09:38:06', '2011-05-26 00:08:40', '2016-06-20 18:54:08',\n",
      " '2010-11-12 22:44:10',\n",
      " ...\n",
      " '2017-02-20 15:49:37', '2019-10-01 20:39:14', '2018-12-01 11:31:04',\n",
      " '2018-03-09 15:37:50', '2018-07-19 22:16:42', '2020-03-16 14:48:39',\n",
      " '2019-09-25 14:24:16', '2018-04-25 20:53:29', '2017-09-01 02:53:49',\n",
      " '2018-04-12 16:35:20']\n",
      "Length: 267, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2011-02-16 01:02:24', '2012-12-15 03:01:55', '2011-06-15 19:35:48',\n",
      " '2010-10-18 12:37:15', '2010-11-16 11:55:02', '2011-07-15 17:25:12',\n",
      " '2010-07-19 12:40:22', '2010-12-13 15:10:30', '2016-05-26 21:24:31',\n",
      " '2010-02-20 11:02:43',\n",
      " ...\n",
      " '2017-02-17 01:54:52', '2019-08-27 16:35:04', '2018-05-29 03:51:08',\n",
      " '2018-03-09 07:41:42', '2018-05-21 20:44:45', '2020-03-12 20:07:11',\n",
      " '2019-09-18 15:33:44', '2018-04-24 19:48:25', '2017-08-20 08:37:31',\n",
      " '2018-02-24 07:00:26']\n",
      "Length: 267, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2011-05-24 22:00:12', '2013-02-22 17:02:11', '2012-02-15 18:29:41',\n",
      " '2011-04-14 07:48:15', '2011-04-08 17:04:44', '2011-08-04 19:01:03',\n",
      " '2011-04-26 22:28:59', '2011-05-26 00:08:40', '2016-06-20 18:54:08',\n",
      " '2010-11-12 22:44:10',\n",
      " ...\n",
      " '2018-07-19 00:20:49', '2019-10-01 20:39:14', '2018-12-01 11:31:04',\n",
      " '2018-07-20 18:49:58', '2018-09-19 14:28:48', '2020-03-16 14:48:39',\n",
      " '2019-09-25 14:24:16', '2019-03-19 10:57:48', '2017-09-01 02:53:49',\n",
      " '2018-04-12 16:35:20']\n",
      "Length: 267, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ D-Tale session launched successfully.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 425\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 425\u001b[0m     planning_df, project_features, project_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mexport_clean_planningphase_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopen_dtale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlanning-phase DataFrame shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, planning_df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProject features DataFrame shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, project_features\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[12], line 406\u001b[0m, in \u001b[0;36mexport_clean_planningphase_df\u001b[0;34m(open_dtale)\u001b[0m\n\u001b[1;32m    403\u001b[0m planning_df \u001b[38;5;241m=\u001b[39m extract_planning_fields(full_df)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# Create project-level features\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m project_features \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_project_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplanning_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Create complete project dataset with resolution metrics\u001b[39;00m\n\u001b[1;32m    409\u001b[0m project_dataset \u001b[38;5;241m=\u001b[39m create_project_dataset(full_df)\n",
      "Cell \u001b[0;32mIn[12], line 243\u001b[0m, in \u001b[0;36mcreate_project_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    240\u001b[0m     agg_dict[col] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# Perform the groupby aggregation\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m project_features \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfields.project.id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfields.project.name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43magg_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Flatten the multi-level columns\u001b[39;00m\n\u001b[1;32m    246\u001b[0m project_features\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(col) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m col \n\u001b[1;32m    247\u001b[0m                            \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m project_features\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/core/groupby/generic.py:1432\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[1;32m   1431\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m-> 1432\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/core/apply.py:190\u001b[0m, in \u001b[0;36mApply.agg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/core/apply.py:423\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m    Result of aggregation.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/core/apply.py:1608\u001b[0m, in \u001b[0;36mGroupByApply.agg_or_apply_dict_like\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1603\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine_kwargs})\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\n\u001b[1;32m   1606\u001b[0m     obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1607\u001b[0m ):\n\u001b[0;32m-> 1608\u001b[0m     result_index, result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1611\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/core/apply.py:496\u001b[0m, in \u001b[0;36mApply.compute_dict_like\u001b[0;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m         results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m key_data\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    497\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(obj\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), op_name)(how, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m func\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    499\u001b[0m     ]\n\u001b[1;32m    500\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keys, results\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/core/apply.py:497\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    493\u001b[0m         results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m key_data\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 497\u001b[0m         \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gotitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m func\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    499\u001b[0m     ]\n\u001b[1;32m    500\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keys, results\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/core/groupby/generic.py:257\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine\n\u001b[1;32m    256\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[0;32m--> 257\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate_multiple_funcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m relabeling:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# columns is not narrowed by mypy from relabeling flag\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/core/groupby/generic.py:362\u001b[0m, in \u001b[0;36mSeriesGroupBy._aggregate_multiple_funcs\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, (name, func) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arg):\n\u001b[1;32m    361\u001b[0m         key \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mOutputKey(label\u001b[38;5;241m=\u001b[39mname, position\u001b[38;5;241m=\u001b[39midx)\n\u001b[0;32m--> 362\u001b[0m         results[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, DataFrame) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m concat\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/core/groupby/generic.py:291\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_constructor(\n\u001b[1;32m    284\u001b[0m         [],\n\u001b[1;32m    285\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    286\u001b[0m         index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mresult_index,\n\u001b[1;32m    287\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    288\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mnkeys \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_agg_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_agg_general(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/core/groupby/generic.py:327\u001b[0m, in \u001b[0;36mSeriesGroupBy._python_agg_general\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: func(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    326\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\n\u001b[0;32m--> 327\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m res \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_constructor(result, name\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_aggregated_output(res)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/core/groupby/ops.py:864\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[0;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39m_values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;66;03m# we can preserve a little bit more aggressively with EA dtype\u001b[39;00m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m#  because maybe_cast_pointwise_result will do a try/except\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m#  with _from_sequence.  NB we are assuming here that _from_sequence\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;66;03m#  is sufficiently strict that it casts appropriately.\u001b[39;00m\n\u001b[1;32m    862\u001b[0m     preserve_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate_series_pure_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m npvalues \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preserve_dtype:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/core/groupby/ops.py:885\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[0;34m(self, obj, func)\u001b[0m\n\u001b[1;32m    882\u001b[0m splitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_splitter(obj, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splitter):\n\u001b[0;32m--> 885\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    886\u001b[0m     res \u001b[38;5;241m=\u001b[39m extract_result(res)\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m initialized:\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;66;03m# We only do this validation on the first iteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/core/groupby/generic.py:324\u001b[0m, in \u001b[0;36mSeriesGroupBy._python_agg_general.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    322\u001b[0m     alias \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39m_builtin_table_alias[func]\n\u001b[1;32m    323\u001b[0m     warn_alias_replacement(\u001b[38;5;28mself\u001b[39m, orig_func, alias)\n\u001b[0;32m--> 324\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\n\u001b[1;32m    327\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39magg_series(obj, f)\n",
      "Cell \u001b[0;32mIn[12], line 213\u001b[0m, in \u001b[0;36mcreate_project_features.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Component usage if available\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfields.components\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m--> 213\u001b[0m     agg_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfields.components\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msublist\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msublist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msublist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Add component and label counts if available\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomponent_count\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "project_root = os.path.abspath(\"..\")  # adjust based on your directory structure\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    \n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from FeatureCleaning.CleanDSDTale import export_clean_df  # Adjust import path as needed\n",
    "import dtale\n",
    "\n",
    "def extract_sprint_name(sprint_str):\n",
    "    \"\"\"\n",
    "    Extract the sprint name from a sprint string.\n",
    "    For example, from a value like:\n",
    "      \"com.atlassian.greenhopper.service.sprint.Sprint@16353814[id=5599,...,name=Sprint 9,...]\"\n",
    "    this returns \"Sprint 9\".\n",
    "    \"\"\"\n",
    "    if not sprint_str or not isinstance(sprint_str, str):\n",
    "        return None\n",
    "    match = re.search(r'name=([^,]+)', sprint_str)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def extract_sprint_from_fixversions(fix_versions):\n",
    "    \"\"\"\n",
    "    Given a list of fixVersions dictionaries, return the first version name containing 'Sprint'.\n",
    "    \"\"\"\n",
    "    if isinstance(fix_versions, list):\n",
    "        for version in fix_versions:\n",
    "            name = version.get(\"name\", \"\")\n",
    "            if \"Sprint\" in name:\n",
    "                return name\n",
    "    return None\n",
    "\n",
    "def add_sprint_field(df):\n",
    "    \"\"\"\n",
    "    Add a standardized 'sprint' column to the DataFrame.\n",
    "    First, try using customfield_10557; if not present, fall back to fixVersions.\n",
    "    \"\"\"\n",
    "    if \"fields.customfield_10557\" in df.columns:\n",
    "        df[\"sprint\"] = df[\"fields.customfield_10557\"].apply(extract_sprint_name)\n",
    "    elif \"fields.fixVersions\" in df.columns:\n",
    "        df[\"sprint\"] = df[\"fields.fixVersions\"].apply(extract_sprint_from_fixversions)\n",
    "    else:\n",
    "        df[\"sprint\"] = None\n",
    "    return df\n",
    "\n",
    "def extract_planning_fields(df):\n",
    "    \"\"\"\n",
    "    From the full cleaned DataFrame, extract only the planning-phase fields.\n",
    "    Enhanced to include all critical planning-phase data for task-level and project-level estimation.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = add_sprint_field(df)\n",
    "    \n",
    "    planning_cols = [\n",
    "        # Basic issue identification\n",
    "        \"key\",                    # Issue key\n",
    "        \"fields.summary\",         # Summary text\n",
    "        \n",
    "        # Issue classification\n",
    "        \"fields.issuetype.name\",  # Type\n",
    "        \"fields.status.name\",     # Status\n",
    "        \"fields.priority.name\",   # Priority\n",
    "        \n",
    "        # People involved\n",
    "        \"fields.assignee.key\",    # Assignee ID\n",
    "        \"fields.creator.key\",     # Creator ID\n",
    "        \"fields.reporter.key\",    # Reporter ID\n",
    "        \n",
    "        # Project context\n",
    "        \"fields.project.id\",      # Project ID\n",
    "        \"fields.project.name\",    # Project name\n",
    "        \n",
    "        # Time information\n",
    "        \"fields.created\",         # Creation date\n",
    "        \n",
    "        # Relationships\n",
    "        \"sprint\",                 # Extracted sprint info\n",
    "        \"issuelinks_total\",       # Total link count\n",
    "        \"has_issuelinks_relates\", # Has 'relates to' links\n",
    "        \"has_issuelinks_cloners\", # Has 'cloners' links\n",
    "        \n",
    "        # Components and labels\n",
    "        \"fields.components\",      # Components\n",
    "        \"fields.labels\",          # Labels\n",
    "        \n",
    "        # For training purposes\n",
    "        \"fields.resolutiondate\"   # Used to calculate resolution time\n",
    "    ]\n",
    "    \n",
    "    # Get all description embedding columns (they start with \"desc_emb_\")\n",
    "    embedding_cols = [col for col in df.columns if col.startswith(\"desc_emb_\")]\n",
    "    \n",
    "    # Combine all desired columns\n",
    "    all_cols = planning_cols + embedding_cols\n",
    "    \n",
    "    # Filter to only include columns that exist in the DataFrame\n",
    "    available_cols = [col for col in all_cols if col in df.columns]\n",
    "    \n",
    "    # Create the base planning DataFrame\n",
    "    planning_df = df[available_cols].copy()\n",
    "    \n",
    "    # Add derived features\n",
    "    \n",
    "    # Component and label counts\n",
    "    if \"fields.components\" in planning_df.columns:\n",
    "        planning_df[\"component_count\"] = planning_df[\"fields.components\"].apply(\n",
    "            lambda x: len(x) if isinstance(x, list) else 0\n",
    "        )\n",
    "    \n",
    "    if \"fields.labels\" in planning_df.columns:\n",
    "        planning_df[\"label_count\"] = planning_df[\"fields.labels\"].apply(\n",
    "            lambda x: len(x) if isinstance(x, list) else 0\n",
    "        )\n",
    "    \n",
    "    # Creation date features\n",
    "    if \"fields.created\" in planning_df.columns:\n",
    "        planning_df[\"created_date\"] = pd.to_datetime(planning_df[\"fields.created\"])\n",
    "        planning_df[\"created_day_of_week\"] = planning_df[\"created_date\"].dt.dayofweek\n",
    "        planning_df[\"created_month\"] = planning_df[\"created_date\"].dt.month\n",
    "        planning_df[\"created_year\"] = planning_df[\"created_date\"].dt.year\n",
    "    \n",
    "    # Calculate resolution time (if available - for training only)\n",
    "    if \"fields.resolutiondate\" in planning_df.columns and \"fields.created\" in planning_df.columns:\n",
    "        planning_df[\"resolution_time\"] = (\n",
    "            pd.to_datetime(planning_df[\"fields.resolutiondate\"]) - \n",
    "            pd.to_datetime(planning_df[\"fields.created\"])\n",
    "        ).dt.total_seconds() / 3600  # Convert to hours\n",
    "        \n",
    "        # Drop the raw resolution date as we now have the derived feature\n",
    "        planning_df = planning_df.drop(columns=[\"fields.resolutiondate\"])\n",
    "    \n",
    "    return planning_df\n",
    "\n",
    "def expand_categorical_features(df):\n",
    "    \"\"\"\n",
    "    Expand categorical distributions stored as dictionaries.\n",
    "    \"\"\"\n",
    "    # Check if DataFrame is empty\n",
    "    if len(df) == 0:\n",
    "        return df\n",
    "    \n",
    "    # Identify columns that contain dictionaries\n",
    "    dict_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object' and df[col].notna().any():\n",
    "            first_valid = df[col].dropna().iloc[0] if not df[col].dropna().empty else None\n",
    "            if isinstance(first_valid, dict):\n",
    "                dict_cols.append(col)\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    \n",
    "    for col in dict_cols:\n",
    "        # Find all unique categories across all projects\n",
    "        all_categories = set()\n",
    "        for d in df[col]:\n",
    "            if isinstance(d, dict):\n",
    "                all_categories.update(d.keys())\n",
    "        \n",
    "        # Create a column for each category\n",
    "        for category in all_categories:\n",
    "            new_col_name = f\"{col}_{category}\"\n",
    "            result_df[new_col_name] = df[col].apply(\n",
    "                lambda x: x.get(category, 0) if isinstance(x, dict) else 0\n",
    "            )\n",
    "        \n",
    "        # Drop the original dictionary column\n",
    "        result_df = result_df.drop(columns=[col])\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def create_project_features(df):\n",
    "    \"\"\"\n",
    "    Aggregate task-level features to create project-level features.\n",
    "    \"\"\"\n",
    "    # Group by project ID and name\n",
    "    agg_dict = {\n",
    "        # Issue counts\n",
    "        'key': 'count',  # Total number of issues\n",
    "    }\n",
    "    \n",
    "    # Add issue type distribution if available\n",
    "    if 'fields.issuetype.name' in df.columns:\n",
    "        agg_dict['fields.issuetype.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add priority distribution if available  \n",
    "    if 'fields.priority.name' in df.columns:\n",
    "        agg_dict['fields.priority.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add status distribution if available\n",
    "    if 'fields.status.name' in df.columns:\n",
    "        agg_dict['fields.status.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Team size metrics if available\n",
    "    if 'fields.assignee.key' in df.columns:\n",
    "        agg_dict['fields.assignee.key'] = lambda x: x.nunique()\n",
    "    \n",
    "    if 'fields.creator.key' in df.columns:\n",
    "        agg_dict['fields.creator.key'] = lambda x: x.nunique()\n",
    "    \n",
    "    # Connectivity metrics if available\n",
    "    if 'issuelinks_total' in df.columns:\n",
    "        agg_dict['issuelinks_total'] = ['sum', 'mean']\n",
    "    \n",
    "    if 'has_issuelinks_relates' in df.columns:\n",
    "        agg_dict['has_issuelinks_relates'] = 'mean'\n",
    "    \n",
    "    # Component usage if available\n",
    "    if 'fields.components' in df.columns:\n",
    "        # Extract component names instead of using the component objects directly\n",
    "        agg_dict['fields.components'] = lambda x: set(\n",
    "            item.get('name', str(item)) if isinstance(item, dict) else str(item)\n",
    "            for sublist in x if isinstance(sublist, list) \n",
    "            for item in sublist\n",
    "        )\n",
    "    \n",
    "    # Add component and label counts if available\n",
    "    if 'component_count' in df.columns:\n",
    "        agg_dict['component_count'] = ['sum', 'mean']\n",
    "    \n",
    "    if 'label_count' in df.columns:\n",
    "        agg_dict['label_count'] = ['sum', 'mean']\n",
    "    \n",
    "    # Add sprint information if available\n",
    "    if 'sprint' in df.columns:\n",
    "        agg_dict['sprint'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add creation time features if available\n",
    "    if 'created_month' in df.columns:\n",
    "        agg_dict['created_month'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    if 'created_day_of_week' in df.columns:\n",
    "        agg_dict['created_day_of_week'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add resolution time statistics if available (for model training)\n",
    "    if 'resolution_time' in df.columns:\n",
    "        agg_dict['resolution_time'] = ['mean', 'median', 'min', 'max', 'std', 'sum']\n",
    "    \n",
    "    # Add description embedding averages if available\n",
    "    embedding_cols = [col for col in df.columns if col.startswith('desc_emb_')]\n",
    "    for col in embedding_cols:\n",
    "        agg_dict[col] = 'mean'\n",
    "    \n",
    "    # Perform the groupby aggregation\n",
    "    project_features = df.groupby(['fields.project.id', 'fields.project.name']).agg(agg_dict)\n",
    "    \n",
    "    # Flatten the multi-level columns\n",
    "    project_features.columns = ['_'.join(col) if isinstance(col, tuple) else col \n",
    "                               for col in project_features.columns]\n",
    "    \n",
    "    # Reset index to convert groupby result to regular DataFrame\n",
    "    project_features = project_features.reset_index()\n",
    "    \n",
    "    # Expand the categorical distributions\n",
    "    project_features = expand_categorical_features(project_features)\n",
    "    \n",
    "    # Add derived metrics\n",
    "    \n",
    "    # Compute team-related metrics if available\n",
    "    if 'fields.assignee.key' in project_features.columns and 'key_count' in project_features.columns:\n",
    "        project_features['issue_to_assignee_ratio'] = (\n",
    "            project_features['key_count'] / project_features['fields.assignee.key'].apply(lambda x: max(1, x))\n",
    "        )\n",
    "    \n",
    "    # Calculate link density if available\n",
    "    if 'issuelinks_total_sum' in project_features.columns and 'key_count' in project_features.columns:\n",
    "        project_features['link_density'] = (\n",
    "            project_features['issuelinks_total_sum'] / project_features['key_count']\n",
    "        )\n",
    "    \n",
    "    # Calculate semantic complexity if embedding columns exist\n",
    "    emb_cols = [col for col in project_features.columns if col.startswith('desc_emb_')]\n",
    "    if emb_cols:\n",
    "        # Variance across embedding dimensions as a complexity measure\n",
    "        project_features['semantic_complexity'] = project_features[emb_cols].var(axis=1)\n",
    "    \n",
    "    # Calculate effort metrics if resolution time is available\n",
    "    if 'resolution_time_mean' in project_features.columns:\n",
    "        # If team size is available\n",
    "        if 'fields.assignee.key' in project_features.columns:\n",
    "            # Total effort (team size × avg resolution time × issue count)\n",
    "            project_features['total_effort'] = (\n",
    "                project_features['fields.assignee.key'] * \n",
    "                project_features['resolution_time_mean'] * \n",
    "                project_features['key_count']\n",
    "            )\n",
    "            \n",
    "            # Effort per issue\n",
    "            project_features['effort_per_issue'] = (\n",
    "                project_features['total_effort'] / project_features['key_count']\n",
    "            )\n",
    "            \n",
    "            # Effort per team member\n",
    "            project_features['effort_per_team_member'] = (\n",
    "                project_features['total_effort'] / project_features['fields.assignee.key']\n",
    "            )\n",
    "    \n",
    "    return project_features\n",
    "\n",
    "def calculate_time_to_resolution(df):\n",
    "    \"\"\"\n",
    "    Calculate time-to-resolution metrics for each project.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The full task-level DataFrame with timestamps\n",
    "        \n",
    "    Returns:\n",
    "        resolution_metrics (pd.DataFrame): DataFrame with resolution time metrics by project\n",
    "    \"\"\"\n",
    "    # Check if required columns exist\n",
    "    if 'fields.resolutiondate' not in df.columns or 'fields.created' not in df.columns:\n",
    "        print(\"Warning: Resolution date or creation date columns missing. Cannot calculate resolution metrics.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Calculate resolution time for each task\n",
    "    df['time_to_resolution'] = (\n",
    "        pd.to_datetime(df['fields.resolutiondate']) - \n",
    "        pd.to_datetime(df['fields.created'])\n",
    "    ).dt.total_seconds() / 3600  # Convert to hours\n",
    "    \n",
    "    # Aggregate to project level\n",
    "    resolution_metrics = df.groupby(['fields.project.id', 'fields.project.name']).agg({\n",
    "        'time_to_resolution': ['mean', 'median', 'min', 'max', 'std', 'sum'],\n",
    "        'fields.resolutiondate': lambda x: x.notna().mean()  # Completion rate\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten the column names\n",
    "    resolution_metrics.columns = [\n",
    "        f\"{col[0]}_{col[1]}\" if isinstance(col, tuple) else col\n",
    "        for col in resolution_metrics.columns\n",
    "    ]\n",
    "    \n",
    "    # Rename completion rate column\n",
    "    resolution_metrics = resolution_metrics.rename(\n",
    "        columns={'fields.resolutiondate_<lambda>': 'completion_rate'}\n",
    "    )\n",
    "    \n",
    "    # Calculate status transition counts if available\n",
    "    if 'changelog_count_status' in df.columns:\n",
    "        status_changes = df.groupby(['fields.project.id', 'fields.project.name']).agg({\n",
    "            'changelog_count_status': ['sum', 'mean']\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Flatten column names\n",
    "        status_changes.columns = [\n",
    "            f\"{col[0]}_{col[1]}\" if isinstance(col, tuple) else col\n",
    "            for col in status_changes.columns\n",
    "        ]\n",
    "        \n",
    "        # Merge with resolution metrics\n",
    "        resolution_metrics = pd.merge(\n",
    "            resolution_metrics, status_changes,\n",
    "            on=['fields.project.id', 'fields.project.name'],\n",
    "            how='left'\n",
    "        )\n",
    "    \n",
    "    return resolution_metrics\n",
    "\n",
    "def create_project_dataset(full_df):\n",
    "    \"\"\"\n",
    "    Create a comprehensive project-level dataset for analysis and modeling.\n",
    "    \n",
    "    Parameters:\n",
    "        full_df (pd.DataFrame): The full task-level DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        project_dataset (pd.DataFrame): Combined project-level dataset\n",
    "    \"\"\"\n",
    "    # Extract planning-phase fields\n",
    "    planning_df = extract_planning_fields(full_df)\n",
    "    \n",
    "    # Create project-level features from planning data\n",
    "    project_features = create_project_features(planning_df)\n",
    "    \n",
    "    # Calculate resolution metrics from full data\n",
    "    resolution_metrics = calculate_time_to_resolution(full_df)\n",
    "    \n",
    "    # Merge features and metrics\n",
    "    if not resolution_metrics.empty:\n",
    "        project_dataset = pd.merge(\n",
    "            project_features, resolution_metrics,\n",
    "            on=['fields.project.id', 'fields.project.name'],\n",
    "            how='left'\n",
    "        )\n",
    "    else:\n",
    "        project_dataset = project_features\n",
    "    \n",
    "    return project_dataset\n",
    "\n",
    "def export_clean_planningphase_df(open_dtale=True):\n",
    "    \"\"\"\n",
    "    Run the full cleaning pipeline to obtain a cleaned task-level DataFrame,\n",
    "    then extract only the planning-phase fields to create a planning-phase dataset.\n",
    "    If open_dtale is True, launch a D-Tale session for interactive exploration.\n",
    "    \n",
    "    Returns:\n",
    "      planning_df (pd.DataFrame): The planning-phase DataFrame with the selected fields.\n",
    "      project_features (pd.DataFrame): Project-level features derived from planning data.\n",
    "      project_dataset (pd.DataFrame): Complete project dataset with resolution metrics.\n",
    "    \"\"\"\n",
    "    # Get the fully cleaned task-level DataFrame\n",
    "    full_df = export_clean_df()  # Call without parameters to match your function signature\n",
    "    \n",
    "    # Extract planning-phase fields\n",
    "    planning_df = extract_planning_fields(full_df)\n",
    "    \n",
    "    # Create project-level features\n",
    "    project_features = create_project_features(planning_df)\n",
    "    \n",
    "    # Create complete project dataset with resolution metrics\n",
    "    project_dataset = create_project_dataset(full_df)\n",
    "    \n",
    "    if open_dtale:\n",
    "        print(\"Launching D-Tale session for planning-phase DataFrame...\")\n",
    "        d_planning = dtale.show(planning_df, ignore_duplicate=True, allow_cell_edits=False, name=\"Planning DataFrame\")\n",
    "        \n",
    "        print(\"Launching D-Tale session for project-level features...\")\n",
    "        d_project = dtale.show(project_features, ignore_duplicate=True, allow_cell_edits=False, name=\"Project Features\")\n",
    "        \n",
    "        print(\"Launching D-Tale session for complete project dataset...\")\n",
    "        d_dataset = dtale.show(project_dataset, ignore_duplicate=True, allow_cell_edits=False, name=\"Project Dataset\")\n",
    "    \n",
    "    return planning_df, project_features, project_dataset\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    planning_df, project_features, project_dataset = export_clean_planningphase_df(open_dtale=True)\n",
    "    \n",
    "    print(\"\\nPlanning-phase DataFrame shape:\", planning_df.shape)\n",
    "    print(\"Project features DataFrame shape:\", project_features.shape)\n",
    "    print(\"Project dataset shape:\", project_dataset.shape)\n",
    "    \n",
    "    # Print key project metrics\n",
    "    if len(project_dataset) > 0:\n",
    "        print(\"\\nProject-Level Feature Summary:\")\n",
    "        print(f\"Number of projects: {len(project_dataset)}\")\n",
    "        \n",
    "        # Display available metrics\n",
    "        key_metrics = [\n",
    "            'key_count', 'fields.assignee.key', 'issue_to_assignee_ratio',\n",
    "            'resolution_time_mean', 'total_effort', 'semantic_complexity'\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nAvailable Metrics:\")\n",
    "        for metric in key_metrics:\n",
    "            if metric in project_dataset.columns:\n",
    "                print(f\"- {metric}\")\n",
    "        \n",
    "        # Show sample of project data\n",
    "        print(\"\\nSample Project Data:\")\n",
    "        display_cols = ['fields.project.name', 'key_count']\n",
    "        for metric in key_metrics:\n",
    "            if metric in project_dataset.columns and metric != 'key_count':\n",
    "                display_cols.append(metric)\n",
    "        \n",
    "        # Only include columns that exist\n",
    "        valid_cols = [col for col in display_cols if col in project_dataset.columns]\n",
    "        if valid_cols:\n",
    "            print(project_dataset[valid_cols].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
